{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Module 17: Time Series Analysis\"\nauthor: \"C. Lau\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\noutput: \n  html_document:\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 4\n---\n\n## Module Objective\n\nAnalyze univariate and multivariate financial and insurance data (incl. asset prices, credit spreads and defaults, interest rates and insurance losses) using appropriate statistical methods\n\nRecommend a specific choice of model based on the results of both quantitative and qualitative analysis of financial or insurance data\n\n***\n\nDiscuss the fitting of distributions to a given set of data and analysis of time series processes\n\nUse of the techniques here to model specific types of risk will be covered in Part 4\n\nSimilar focus and note as the previous section\n\n## Definitions\n\n### Strict Stationarity\n\nA process $\\{X_t : t= 1,2,...,T\\}$ is said to be ***strictly stationary*** if:\n\n* The **joint distribution**:  \n\n    $X_r,X_{r+1},...,X_s$ and \n    \n    $X_{r+k}, X_{r+k+1},...,X_{s+k}$\n    \n    are **identical** for all $r, s, k \\in \\mathbb{Z}$\n\n### Weak and Covariance Stationarity\n\nA process $\\{X_t : t= 0,1,2,...,T\\}$ is said to be **covariance stationary** (or **weakly stationary**) if:\n\n1. The **mean** of the process $m(T) = \\mathrm{E}(X_t)$ is **constant**, and \n\n2. The **covariance** of the process:  \n\n    $\\mathrm{Cov}(X_r, X_{r+k}) = \\mathrm{E}\\left[\\left(X_r - m(r)\\right)\\left(X_{r+k} - m(r+k)\\right)\\right]$  \n\n    **depends only on the time different** $k$\n\n***Weakly stationary*** of order *n* $\\Rightarrow$ the moments of subsets of the process are **defined and equal** *up to* the *n*^th^ moment\n\n***Importance of stationarity*** in modeling financial time series\n\n* Stationarity means that the **statistical properties** of the process (incl. the moments and the relationships between observations in different periods) **stay the same over time**\n\n* Important as it determines the **extend to which we can use past data** to try to model the future\n\n* `Strict stationarity` is very restrictive and is **unlikely true in real world data**\n\n* The requirement can be relaxed and we **can still analyze the data under weak stationarity**\n\n**Finite variance** process strict stationarity $\\Rightarrow$ Weak stationarity\n\n**Infinite variance** process strict stationarity $\\Rightarrow \\!\\!\\!\\!\\!\\! \\big/$ Weak stationarity\n\n### White Noise\n\nA process $\\{\\epsilon_t : t = 0,1,2,...,T\\}$ is a ***white noise process*** if it is: \n\n1. **Covariance stationary**\n\n2. $\\forall$ $t \\in \\mathbb{Z}$: \n\n    a. $\\mathrm{Corr}(\\epsilon_t,\\epsilon_{t+k}) = \\begin{cases} 1 & \\text{if } k = 0 \\\\ 0 & \\text{if } k \\neq 0 \\\\ \\end{cases}$\n\n    b. $m(t) = \\mathrm{E}(\\epsilon_t) = 0$\n    \n***White noise process***\n\n* Every element is **uncorrelated** with any previous observations and the process oscillates randomly **around zero**\n\n* Important concept for modeling financial time series as many financial processes have a random element that cannot be known in advance (which we call white noise)\n\n***Strict White Noise***\n\nA white noise process $\\{\\epsilon_t : t = 1,2,...,T\\}$ is a ***strict white noise*** process if it is:\n\n1. A set of *iid* r.v. with finite variance\n\n2. (Typically) assume $N(0,\\sigma^2)$\n\n### Trend and Difference Stationarity\n\n***Trend Stationary***\n\n* Where the observations oscillate randomly around a **steadily changing value** which is a function of time only\n\n    e.g. $X_t = \\alpha_0 + \\alpha_1 t + \\epsilon_t$\n\n***Difference Stationarity and Integrated Processes***\n\n* We can difference successive observations to generate a new process which may be stationary (if the observed TS is not) and model the new process\n\n***Integrated process of order $d$*** ($I(d)$ process)\n\n* One where the process needs to be differenced $d$ times before the result $\\Delta^d X_t$ is covariance stationary\n\n    * (Difference stationary process) Integrated process of order 1 ($I(1)$ process) is one where $\\Delta X_t = X_t - X_{t-1}$ is covariance stationary\n\n    * Integrated process of order 2 ($I(2)$ process) is one where $\\Delta^2 X_t = \\Delta X_t - \\Delta X_{t-1}$ is covariance stationary (but $\\Delta X_t$ is not)\n\n***Dickey-Fuller test*** can help **distinguish** between `trend` and `difference stationary processes`\n\n* Regresses the first difference of the observations onto the preceding observation and a time trend:\n\n    e.g. $\\Delta X_t = \\alpha_0 + \\alpha_1t + \\alpha_2X_{t-1} + \\epsilon_t$\n\n    * Test statistic $\\alpha_2 \\big/ s.e.(\\alpha_2)$ is compared to the critical values determined by Dickey-Fuller\n\n    * If $\\alpha_2$ is (statistically) significantly different to 0 then the series is an integrated process (else trend stationary)\n    \n## Inter-Temporal Links\n\nModel processes where **observations depend on previously observed values**\n\n### Autoregressive\n\n$AR(1)$ process:\n\n* $X_t = \\alpha_0 + \\alpha_1X_{t-1} + \\epsilon_t$\n\n* Each value depends only on the prior value (plus random error)\n\n* To **avoid negative values**:\n\n    $X_t = \\alpha_0 + \\alpha_1 X_{t-1} + \\sqrt{X_{t-1}}\\epsilon_t$\n\n$AR(p)$ process (or p-period, lag p):\n\n* $X_t = \\alpha_0 + \\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2} + \\dots +  \\alpha_{p} X_{t-p} + \\epsilon_t$\n\n* Value depends on $p$ previous values (plus random error)\n\n***Influence of $\\alpha_1$ on $AR(1)$ process***\n\n* If $\\mid \\alpha_1 \\mid < 1$\n\n    $\\hookrightarrow$ Process is **mean reverting** (\\therefore covariance stationary)\n\n    * Mean: $\\dfrac{\\alpha_0}{1-\\alpha_1}$\n\n    * Variance: $\\dfrac{\\sigma^2}{1 - \\alpha_1^2}$; $\\sigma^2$ is the variance of the white noise ($\\epsilon$)\n\n* If $\\mid \\alpha_1 \\mid > 1$\n\n    $\\hookrightarrow$ Process becomes **unstable**\n\n* If $\\mid \\alpha_1 \\mid = 1$\n\n    $\\hookrightarrow$ Process is a **random walk**\n\n    * If in addition, $\\alpha_0 \\neq 0$\n    \n        $\\hookrightarrow$ Process is a random walk **with drift** (this is difference stationary)\n\n***Condition for $AR(p)$ to be (at least) weakly stationary***\n\n* Length of the $p$-dimensional vector containing the roots ($z$) of the following polynomial expression must be > 1:\n\n    $f(z) = 1 - \\alpha_1 z - \\alpha_2 z^2 - \\dots - \\alpha_p z^p = 0$\n\n    e.g. For $p=1$: $f(z) = 1 - \\alpha_1 z = 0$ and the root is $z = \\dfrac{1}{\\alpha_1}$\n\n* **Requirement for covariance stationary**: is that **the root lies outside the unit circle**\n\n    For $p=1$ this will be if $\\mid \\alpha_1 \\mid < 1$\n\n### Moving Average\n\n$MA(q)$ process:\n\n* $X_t = \\epsilon_t + \\beta_1\\epsilon_{t-1} + \\dots + \\beta_q \\epsilon_{t-q}$\n\n* A $q$-period (or lag q) moving average process\n\n***Durbin-Watson statistic*** ($d$)\n\n* Used to **test for serial correlation** (i.e. correlation between the values of the process at adjacent times) in an $MA$ process:\n\n    $d = \\dfrac{\\sum \\limits_{t=2}^T (\\epsilon_t - \\epsilon_{t-1})^2}{\\sum \\limits_{t=1}^T \\epsilon_t^2}$\n\n* Null hypothesis ($H_0$) is that $d=2$ in which case there is no serial correlation\n\n    If $\\epsilon_t$ and $\\epsilon_{t-1}$ are uncorrelated, $\\epsilon_t - \\epsilon_{t-1}$ should have twice the variance of $\\epsilon_t$ itself\n\n* There are two **critical value** of $d$ that are used:\n\n    A lower ($d_L$) and upper ($d_U$) limit depending on the level of significance\n\n| Test Statistic | Value | Interpretation |\n| -------------- | --------------------- | ------------------------|\n| $d$ | $d_L$ > Test statistic | Statistically significant **positive** serial correlation |\n| $d$ | $d_L$ < Test statistic < $d_U$ | Inconclusive |\n| $d$ | Test statistic > $d_U$ | No statistically significant positive serial correlation |\n| $4-d$ | $d_L$ > Test statistic | Statistically significant **negative** serial correlation |\n| $4-d$ | $d_L$ < Test statistic < $d_U$ | Inconclusive |\n| $4-d$ | Test statistic > $d_U$ | No statistically significant negative serial correlation |\n\n### Integrated Moving Average Processes\n\nCombining $AR(p)$ and $MA(q)$ results in an integrated moving average or $ARMA(p,q)$ process:\n\n* $X_t = \\left(\\alpha_0 + \\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2} + \\dots +  \\alpha_{p} X_{t-p}\\right) + \\left( \\epsilon_t + \\beta_1\\epsilon_{t-1} + \\dots + \\beta_q \\epsilon_{t-q}\\right)$\n\n$ARIMA(p,d,q)$ process is one where the $I(d)$ process is an $ARMA(p,q)$ process:\n\n* $\\Delta^d X_t = \\left(\\alpha_0 + \\alpha_1 \\Delta^d X_{t-1}+ \\alpha_2 \\Delta^d X_{t-2} + \\dots +  \\alpha_{p} \\Delta^d X_{t-p}\\right) + \\left( \\epsilon_t + \\beta_1\\epsilon_{t-1} + \\dots + \\beta_q \\epsilon_{t-q}\\right)$\n\n### Fitting ARIMA Models\n\nAutocorrelation functions (ACFs) are the most commonly used statistic in time series analysis\n\n***Definitions***:\n\n* Let the mean of the stationary process = $\\mathrm{X_t} = \\mu$\n\n* **Autocovariance function**\n\n    $\\gamma_h = \\mathrm{Cov}(X_t,X_{t-h}) = \\mathrm{E}\\left[(X_t - \\mu)(X_{t-h} - \\mu)\\right] = \\mathrm{E}\\left[X_tX_{t-h}\\right]-\\mathrm{E}[X_{t}]\\mathrm{E}[X_{t-h}]$\n\n    $\\gamma_0 = \\mathrm{Var}(X_t)$\n\n* **Autocorrelation function (ACF)**\n\n    $\\rho_h = \\mathrm{Corr}(X_t, X_{t-h}) = \\dfrac{\\gamma_h}{\\gamma_0}$\n\n* **Partial autocorrelation function (PACF)**\n\n    $\\{\\phi_h : h=1,2,...\\}$ $\\triangleq$ the **conditional correlation** of $X_{t+h}$ with $X_t$ *given* $X_{t+1},...,X_{t+h+1}$\n\n    * PACF is defined for **positive lags only** (unlike autocovariance and autocorrelation)\n\n    * PACF may be derived as the coefficient $\\phi_{h,h}$ when determining $\\mathrm{min} \\: \\mathrm{E}\\left[(X_t - \\phi_{h,1}X_{t-1} - \\phi_{h,2}X_{t-2} - \\dots - \\phi_{h,h}X_{t-h})^2\\right]$\n\n        * Explanation of the expression above:\n\n            Suppose that at time $t-1$ you are trying to estimate $X_t$ but that you are going to limit your choice of estimatetor to linear functions of the $k$ previous values $X_{t-k},...,X_{t-1}$\n\n            The most general linear estimator will be of the form: $\\phi_{k,1}X_{t-1} + \\phi_{k,2}X_{t-2} + \\dots + \\phi_{k,k}X_{t-k}$ where $\\phi_{k,i}$ are constants\n\n            We can choose the coefficients to minimize the MSE, which is the expression given above\n\n            The partial autocorrelation for lag $k$ is then the weight that you assign to the $X_{t-k}$ term\n\n#### Box-Jenkins Method\n\nDeduce information about at time series from a **plot of the sample ACF** ${r_h}$ (a function of the lag): **correlogram**\n\n$r_h = \\dfrac{\\sum \\limits){t=h+1}^T (X_t - \\bar{X})(X_{t-h} - \\bar{X})}{\\sum \\limits_{t=1}^T (X_t - \\bar{X})^2}$\n\n* Separate correlograms might be constructed for a variety of degrees of integration (e.g. d = 0, 1, 2) \n\n* So as to check visually for both `serial correlation`, `degree of integration` and the `form of model fit`\n\n***Correlograms Interpretation***\n\nObserving patterns in the values of the sample ACF and sample PACF can assist in determining an appropriate model to fit\n\n| | $AR(p)$ | $MA(q)$ | $ARMA(p,q)$ |\n| ------ | ------------ | ------------------ | ------------------|\n| ACF | Tails off | Cuts off after lag $q$ | Tails off, but with a kink at lag $q$ |\n| PACF | Cuts off after lag $p$ | Tails off | Tails off, but with a kink at lag $p$ |\n\ne.g. observing that the ACF suddenly falls below a significant level, and can therefore be regarded as insignificant (referred to as \"cutting off\"), after lag 3 might indicate an $MA(3)$ process\n\n***Example***\n\n![](figures/figure-17.01.png)\n\n* The above data sample shows the PACF 'cut off' after lag 1\n\n* Suggest the process might be described by an $AR(1)$ model\n\n* If the model is to be fitted then the sample ACF indicates that $\\alpha_1 = 0.646$\n\n#### Testing the Fit\n\n1. Use **test statistics** to compare between models under consideration\n\n    (e.g. AIC and BIC in Module 19)\n\n2. Can also consider error terms (**residuals**) arising after having fitted a particular model\n\n    The error terms can be tested to see whether there is any residual structure (i.e. whether or not they are white noise)\n\n    e.g. $AR(1)$ model residual at time $t$: $\\hat{\\epsilon_t} = X_t - \\hat{\\alpha_0} - \\hat{\\alpha_1}X_{t-1}$\n\n    * The first residual when $t=0$, $\\hat{\\epsilon_0}$ is generally set to 0 or $X_{-1}$ is set to $\\bar{X}$\n\n### Predicting with ARIMA\n\nIf the values up to time $t$ have been observed then the next value of a series can be predicted using a model with appropriate (fitted) parameters\n\n***Example*** with $ARMA(1,1)$\n\nEstimated value of the process at time $t+1$:\n\n$\\hat{X}_{t+1} = \\hat{\\alpha}_0 + \\hat{\\alpha}_1 X_t + \\hat{\\epsilon}_{t+1} + \\hat{\\beta}_1 \\hat{\\epsilon}_t$\n\n* $X_t$: observed\n\n* $\\hat{\\epsilon}_{t}$: calculated\n\n* $\\hat{\\epsilon}_{t+1}$: estimated (i.e. generated from a normal distribution)\n\nWorking forward iteratively, taking expectations and noting that $\\mathrm{E}[\\hat{\\epsilon_{t+k}}] = 0$ yields:\n\n$\\begin{align}\n  \\mathrm{E}[X_{t+h}] &= \\hat{\\alpha}_0 \\left(1 + \\hat{\\alpha}_1 + \\dots + \\hat{\\alpha}^{h-1}_1 \\right) + \\hat{\\alpha}^h_1 X_t + \\hat{\\alpha}_1^{h-1} \\hat{\\beta}_1 \\hat{\\epsilon}_t \\\\\n  &= \\hat{\\alpha}_0 \\sum \\limits_{i=0}^{h-1} \\hat{\\alpha}^i_1 + \\hat{\\alpha}_1^h X_t + \\hat{\\alpha}_1^{h-1} \\hat{\\beta}_1 \\hat{\\epsilon}_t \\\\\n\\end{align}$\n\n### Modeling Specific Features of the Data\n\n#### Seasonality\n\nSeasonality can be modeled with dummy (**indicator**) variables\n\ne.g. process with 4 seasons: $X_t = \\alpha_0 + \\alpha_1 d_1 + \\alpha_2 d_2 + \\alpha_3 d_3 \\alpha_4 t + \\epsilon_t$\n\n* $d_1$ takes the value 1 if $X_t$ is an observation from the *i*^th^ quarter and 0 otherwise\n\n* No $d_4$ as the 4^th^ quarter is treated as the baseline and this is adjusting up or down to get values for Q1, Q2, Q3\n\n#### Structural Breaks\n\n***Step Change***\n\nJump in the value of the process can be modeled using a **Poisson variable**\n\n$\\Delta X_t = (\\alpha_0 - \\lambda k) + \\epsilon_t + k P_t(\\lambda)$\n\n* $P_t(\\lambda)$ is a Poisson variable with mean $\\lambda$\n\n* $k$ is the size of the jump when it occurs (assumed constant)\n\n* $\\epsilon$ is the error term (perhaps assumed normally distributed with variance $\\sigma^2$)\n\n* Note that $-\\lambda k$ appears in order to maintain the average drift rates as $\\alpha_0$\n\n***Altered Rate of Change***\n\nParameters might be time dependent:\n\n1. `Drift` ($\\alpha_0$) and/or `degree of mean reversion` ($\\alpha_1$) in the $AR$ model ($X_t = \\alpha_0 + \\alpha_1 X_{t-1} + \\epsilon_t$) might be **dependent upon time**\n\n2. Alternatively, the `rate of trend` ($\\alpha_1$) in a trend stationary model ($X_t = \\alpha_0 + \\alpha_1 t + \\epsilon_t$) might be also **dependent upon time**\n\nTime dependencies can be difficult to *determine* by **visual inspection** of the data\n\n* If such a structural break is suspected then the series might be split into two, either side of the suspected break, and a Chow test performed\n\n***Chow Test***\n\nChow test involves fitting models and calculating the sum of squared residuals (SSR)\n\n1. Calculate the $SSR$ for fitting a model to the **whole series**\n\n2. Calculate $SSR_1$ and $SSR_2$ by fitting the **same model** but with **different parameters** to the 2 sub-series (either side of the suspected break)\n\n3. **Test statistics**:\n\n    $CT = \\dfrac{\\left(SSR - (SSR_1 + SSR_2)\\right)\\big/k}{(SSR_1 + SSR_2)\\big/(N_1 + N_2 - 2k)}$\n\n    * $N_1$ and $N_2$ are the number of observations in the 2 subseries\n\n    * $k$ is the number of parameters in the model\n\n4. Test statistics has an *F*-distribution with $k$ and $N_1 + N_2 -2k$ d.f.\n\n    $H_0$ is that there is no structural break  \n    (i.e. the parameters of the models fitted to the subseries are not significantly different from that of the model fitted to the complete series)\n\n## ARCH and GARCH\n\nModels for series that exhibit **heteroskedasticity**\n\n### ARCH\n\n***Autoregressive conditional heteroskedasticity*** (ARCH)\n\n* Based on **strictly stationary** `white noise process` ($Z_t$) with 0 mean and unit s.d.\n\n* ARCH process is constructed so that the s.d. ($\\sigma_t$) varies over time\n\n***Definition*** of $ARCH(p)$ process\n\nThe process $\\{X_t : t= ..., -2, -1, 0,1,2,...\\}$ is an $ARCH(p)$ process if:\n\n$X_t = \\epsilon_t = \\sigma_t Z_t$\n\n* $\\sigma_t^2 = \\alpha_0 + \\sum \\limits_{i=1}^p \\alpha_i X_{t-i}^2$\n\n* $\\alpha_0 > 0$\n\n* $\\alpha_i \\geq 0$ for $i = 1,...,p$\n\n* $\\{Z_t : t = ...,-2, -1, 0, 1, 2, ...\\}$ is strict white noise with mean 0 and variance 1\n\n***Heteroskedasticity and volatility clustering***\n\n* Given the $\\{X_t\\}$ is weakly stationary \n\n    $\\hookrightarrow$ $\\mathrm{E}(X^2_t) < \\infty$ and\n\n    $\\hookrightarrow$ $\\mathrm{Var}\\left(X_t \\mid F_{t-1}\\right) = \\mathrm{E}\\left(\\sigma_t^2 Z_t^2 \\mid F_{t-1}\\right) = \\sigma^2_t \\mathrm{Var}(Z_t) = \\sigma^2_t = \\alpha_0 + \\sum \\limits_{i=1}^p \\alpha_i X_{t-i}^2$\n\n    * $F_{t-1}$ is the history of the process up to time $t-1$\n\n* Conditional variance of $\\{X_t\\}$ is changing over time\n\n    * Property of heteroscedasticity\n\n    * Function of the previous squared values of the process\n\n    * Large change in the value of the process is often followed by a period of high volatility\n    \n        $\\therefore$ Model incorporates the feature of **volatility clustering**\n\n* Although the conditional variance is changing, we don't know in advance when periods of high and low future volatility will occur \n    \n    $\\therefore$ Still possible for the $ARCH$ process to be stationary\n\n***Condition for weakly stationary***\n\n* $ARCH(1)$ is weakly (covariance) stationary white noise process iff $\\alpha_1 < 1$\n\n    Variance of the process is $\\dfrac{\\alpha_0}{1-\\alpha_1}$\n\n* **Condition** for $ARCH(p)$ to be **weakly stationary**\n\n    Roots of the polynomial $f(z) = 1 - \\alpha_1 z - \\alpha_2 z^2 - \\dots - \\alpha_p z^p = 0$ must lie outside the unit circle\n\n***Condition for strictly stationary***\n\n* Result for weak stationarity of an $ARCH(1)$ process above does not depend on the distribution of $Z_t$ but the result for strict stationarity does\n\n* If $Z_t \\sim N(0,1)$ then the condition for a strictly stationary solution is approximately $\\alpha_1 < 2e^{\\eta} \\approx 3.562$ where $\\eta$ is the Euler-Mascheroni constant (= 0.57721..., usually denoted by gamma)\n\n* For $m\\geq 1$ the strictly stationary $ARCH(1)$ process has finite moments of order $2m$ iff $\\mathrm{E}(Z^{2m}_t) < \\infty$ and $\\alpha_1 < \\left[\\mathrm{E}(\\left(Z_t^{2m} \\right)\\right]^{-\\frac{1}{m}}$\n\n    And the XS kurtosis is $\\kappa = \\dfrac{\\mathrm{E}(Z^4_t)(1-\\alpha^2_1)}{1-\\alpha^2_1\\mathrm{E}(Z^4_t)} - 3$\n\n### GARCH {#GARCH}\n\n***Generalized ARCH***\n\n* Volatility is now allowed to depend on *previous* values of **volatility** as well as *previous* **values of the process**\n\n* Periods of high volatility **tend to last for a long time**\n\n    * Also the case for high order $ARCH$ but $GARCH$ produce the effect with a lower number of parameters\n    \n***Definition***\n\nThe process $\\{X_t : t = ..., -2, -1, 0, 1, 2, ...\\}$ is a $GRACH(p,q)$ process if:\n\n$X_t = \\sigma_t Z_t$\n\n* $\\sigma_i = \\sqrt{\\alpha_0 + \\sum \\limits_{i=1}^p \\alpha_i X^2_{t-i} + \\sum \\limits_{j=1}^q \\beta_j \\sigma^2_{t-j}}$\n\n* $\\alpha_0 > 0$\n\n* $\\alpha_i \\geq 0$ for $i = 1,...,p$\n\n* $\\beta_j \\geq 0$ for $j = 1,...,q$\n\n* $\\{Z_t : t = ...,-2, -1, 0, 1, 2, ...\\}$ is strict white noise with mean 0 and variance 1\n\n***Condition for weakly stationary***\n\n* $GARCH(1,1)$ is weakly stationary iff $\\alpha_1 + \\beta_1 <1$\n\n    Variance of the process is $\\dfrac{\\alpha_0}{1-\\alpha_1 - \\beta_1}$\n\n* **Condition** for $GARCH(p,q)$ to be **weakly stationary**\n\n    If $\\sum \\limits_{i=1}^p \\alpha_i + \\sum \\limits_{j=1}^q \\beta_j <1$\n\n#### Integrated GARCH\n\n***IGARCH***\n\n* Occurs when :\n\n    $\\sum \\limits_{i=1}^p \\alpha_i + \\sum \\limits_{j=1}^q \\beta_j = 1$\n\n* For an $IGRACH(1,1)$ process $\\beta_1 = 1 - \\alpha_1$\n\n    $\\hookrightarrow$ $\\Delta X_t^2 = \\alpha_0 - (1- \\alpha_1)V_{t-1} + V_t$\n\n* sSimilar to $ARIMA(0,1,1)$ model for $\\{X_t^2\\}$\n\n    Although $\\{V_t\\}$ is **not white noise**\n\n#### ARMA model with GARCH errors\n\n1. First fit the model to $ARMA$\n\n2. Then fit a $GARCH$ model to the resulting residuals (if residuals are not white noise)\n\n***Definition***\n\n$\\{X_t\\}$ is an $ARMA(p,q)$ with $GARCH(r,s)$ errors if:\n\n* Weakly stationary\n\n* $X_t = \\mu_t + \\sigma_t Z_t$ where $\\{Z_t\\}$ is strict white noise with mean 0 and variance 1\n\n* $\\mu_t = \\mu + \\sum \\limits_{i=1}^p \\varphi_i(X_{t-i} - \\mu) + \\sum \\limits_{j=1}^q \\theta_j (X_{t-j} - \\mu_{t-j})$\n\n* $\\sigma_t^2 = \\alpha_0 + \\sum \\limits_{i=1}^r \\alpha_i (X_{t-i} - \\mu_{t-i})^2 + \\sum \\limits_{j=1}^s \\beta_j \\sigma^2_{t-j}$\n\n*With*:\n\n* $\\alpha_0 >0$, $\\alpha_i \\geq >0$ for $i = 1,2,...,r$\n\n* $\\beta_j \\geq 0$ for $j=1,2,...,s$\n\n* $\\sum \\limits_{i=1}^r \\alpha_i + \\sum \\limits_{j=1}^s \\beta_j < 1$\n\n### Fitting GARCH\n\nMost commonly used technique is the **method of maximum likelihood**\n\n***Challenge***\n\n1. PDF of $X_0$ is not known\n\n    We get around this by considering the conditional likelihood given $X_0 = x_0$\n\n2. $\\sigma_0$ is not actually observed\n\n    We can choose a starting value for it, which might be the sample s.d. of $X_1,...,X_T$ or just 0\n\n***Testing the fit***\n\nAs with $ARMA$ it is good to check the goodness of fit of a GARCH model by **examining the residual**\n\n* e.g. for $ARAM$ with $GARCH$ errors, the $GARCH$ errors will be of the form $X_t = \\epsilon_t = \\sigma_t Z_t$\n\nMust *distinguish* between ***unstandardized and standardized residuals***\n\n* **Unstandardized residuals**: \n\n    Residuals $\\hat{\\epsilon}_1,...,\\hat{epsilon}_T$ after having fitted the $ARMA$ part of the model\n\n    * If the $ARMA$ model and parameteriztion are correct then the unstandardized residuals should look like a pure $GARCH$ process\n\n* **Standardized residuals**:\n\n    Reconstructed realizations of the strict white noise process $\\{Z_t\\}$ that is assumed to drive the $GARCH$ part of the model\n\n    Calculated using:\n    \n    $\\hat{Z}_t = \\dfrac{\\hat{\\epsilon_t}}{\\hat{\\sigma}_t}$\n\n    Where: $\\hat{\\sigma^2_t} = \\hat{\\alpha}_0 + \\sum \\limits_{i=1}^{p_2} \\hat{\\alpha}_i \\hat{\\epsilon}_{t-i}^2 + \\sum \\limits_{j=1}^{q_2} \\hat{\\beta}_j\\hat{\\sigma}_{t-j}^2$\n\n    Need initial values to use the equations above\n\n    * Can set starting $\\hat{\\epsilon}_t =0$ and $\\hat{\\sigma}_t = 0$ (or sample s.d. of the observed time series)\n\n    The standardized residuals should behave like strict white noise\n\n    * Test with correlograms and statistical test (portmanteau test and the turning point test)\n\n    * If there is insufficient evidence to reject the hypothesis of strict white noise, the validity of the distribution used to construct the likelihood function (i.e. $f$, the PDF of $Z_t$) can be investigated using **QQ plots** and **goodness of fit test** for the `normal` or `scaled t` distribution\n\n### GARCH Based Volatility Prediction\n\n**Data**: $X_{t-n+1},...,X_t$ from a $GARCH$ model\n\n**Predict**: Values of $\\sigma_{t+h}$ for $h \\geq 1$ from a fitted $GARCH(1,1)$ model\n\n***Step 1***\n\n* Let $F_t$ be the **infinite history** of the process up to time $t$ \n* Assume that the model is weakly stationary\n\n    $\\hookrightarrow$ $\\mathrm{E}(X^2_t) = \\mathrm{E}(\\sigma^2_t) < \\infty$:\n\n*Then*:\n\n$\\begin{align}\n  \\mathrm{E}(X^2_{t+1} \\mid F_t) &= \\mathrm{E}(\\sigma^2_{t+1}Z^2_{t+1} \\mid F_t) \\\\\n  &= \\mathrm{E}\\left[(\\alpha_0 + \\alpha_1 x^2_t +\\beta_1 \\sigma_t^2)Z^2_{t+1} \\mid F_t\\right] \\\\\n  &= (\\alpha_0 + \\alpha_1 x^2_t + \\beta_1\\sigma_t^2)\\mathrm{E}(Z^2_{t+1} \\mid F_t) \\\\\n  &= \\alpha_0 + \\alpha_1 x^2_t + \\beta_1 \\sigma^2_t \\\\\n\\end{align}$\n\n* *Since* $\\mathrm{E}(Z^2_{t+1} \\mid F_t) = \\mathrm{E}(Z^2_{t+1}) = \\mathrm{Var}(Z^2_{t+1}) + [\\mathrm{E}(Z_{t+1})]^2 = 1$\n\n* *Also note* $\\mathrm{E}(X^2_{t+1} \\mid F_t) = \\sigma^2_{t+1}$\n\n***Step 2***\n\n* Problem with the above is that we don't have infinite history of the process\n\n    * For $\\sigma^2_t$:\n    \n        Approximate using the residual equations in the [GARCH section](#GARCH)\n  \n    * For $\\sigma^2_{t+1}$:\n    \n        Estimate with the approximate forecast of $X_{t+1}^2$\n\n        $\\hat{\\sigma}^2_{t+1} = \\hat{\\mathrm{E}}(X_{t+1}^2 \\mid F_t) = \\alpha_0 + \\alpha_1 X^2_t + \\beta_1 \\hat{\\sigma}^2_t$\n    \n    * Use above equation to estimate volatility one step ahead recursively\n\n* To estimate $X_{t+h}$ and $\\sigma_{t+h}$ given the information up to $t$\n\n    Both $X_{t+h}$ and $\\sigma_{t+h}$ are r.v. and:\n\n    $\\begin{align}\n        \\mathrm{E}(X^2_{t+h} \\mid F) &= \\mathrm{E}(\\sigma^2_{t+h} \\mid F_t) \\\\\n        &= \\alpha_0 + \\alpha_1 \\mathrm{E}(X^2_{t+h-1} \\mid F_t) + \\beta_1 \\mathrm{E}(\\sigma^2_{t+h-1} \\mid F_t) \\\\\n        &= \\alpha_0 + (\\alpha_1 + \\beta_1)\\mathrm{E}(X^2_{t+h-1} \\mid     F_t) \\\\\n    \\end{align}$\n  \n* So in general:\n\n    $\\mathrm{E}(X^2_{t+h} \\mid F) = \\alpha_0 \\sum \\limits_{i=0}^{h-1}(\\alpha_1 + \\beta_1)^i + (\\alpha_1 + \\beta_1)^{h-1} (\\alpha_1 X^2_t + \\beta_1 \\sigma^2_t)$\n\n    * Use this formula by replacing $\\sigma^2_t$ by $\\hat{\\sigma}^2_t$\n\n## Data Frequency\n\n***Problem with time series***: insufficient data may be available to generate statistics based on a long timescale (e.g. annual volatility)\n\n* In such case we may need to **scale up** statistics\n\n    (e.g. estimate annual volatility from monthly volatility)\n\n* If data is available from $N$ groups:\n\n    Each of which is ***iid normal*** with:\n    \n    * Means $\\bar{\\mathbf{X}}$\n    \n    * Covariances given by the $N \\times N$ matrix $\\boldsymbol{\\Sigma}$\n    \n    $\\hookrightarrow$ `means` and `covariances` working over a timescale $T$ times as long are:\n    \n    * $T \\bar{\\mathbf{X}}$ and \n    \n    * $T\\boldsymbol{\\sigma}$\n\n* E.g. if data is available based on a monthly timescale then multiplying the means and covariance by 12 will provide the corresponding annual statistics\n\n* Similarly if $\\bar{\\mathbf{X}} = \\mathbf{0}$\n\n    $\\hookrightarrow$ Aggregate volatility for the longer timescale can be derived as:\n    \n    $\\sqrt{T}$ multiplied by the corresponding volatility for the shorter timescale\n\n* Applying to VaR\n\n    Annual VaR might be derived from the monthly VaR by multiplying by $\\sqrt{12}$  \n    (i.e. the square root of time rule)\n\nSuch scaling becomes **inaccurate if the data is not *iid* normal**\n\n* e.g. if its skewed or there is serial correlation\n\n* Better approach in such situations is to ***parameterize as stochastic model***\n\n    Using the shorter timescale data and then to derive the longer timescale statistics from a set of simulations",
    "created" : 1474417620302.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1416251627",
    "id" : "5F292667",
    "lastKnownWriteTime" : 1474427042,
    "last_content_update" : 1474427042456,
    "path" : "~/Git Repos/Exam ST9 2016 Notes/4. Risk Modeling/17-Time-Series-Analysis.Rmd",
    "project_path" : "4. Risk Modeling/17-Time-Series-Analysis.Rmd",
    "properties" : {
        "docOutlineVisible" : "1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}