{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Module 18: Copulas\"\nauthor: \"C. Lau\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\noutput: \n  html_document:\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 4\n---\n\n## Module Objective\n\nDemonstrate understanding of the use of copulas as part of the process of modeling multivariate risks, including recommendation of an appropriate copula\n\n***\n\nERM is interested in all the risk an org faces and ways they interact with each other\n\nModule focus on the theory and application of copulas\n\nUse of the techniques here to model specific types of risk will be covered in part 5\n\n## Recall: Prereq\n\n***PDF and CDF***\n\nFor any r.v. $X$:\n\n* PDF: $f(x) = \\Pr(X=x)$\n* CDF: $F(x) = \\Pr(X \\leq x)$\n\nBoth have a range of $[0,1]$\n\n***Marginal PDF***\n\n$P(X = x) = \\sum \\limits_y P(X=x, Y=y)$\n\n$f_X(x) = \\int \\limits_y f_{X,Y}(x,y)dy$\n\n***Conditional PDF***\n\n$P(X=x \\mid Y=y) = \\dfrac{P(X=x,Y=y)}{P(Y=y)}$\n\n$f_{X \\mid Y = y}(x,y) = \\dfrac{f_{X,Y}(x,y)}{f_Y(y)}$\n\n***Expectation***\n\n$\\mathrm{E}[g(X,Y)] = \\sum \\limits_x \\sum \\limits_y g(x,y)P(X=x,Y=y)$\n\n$\\int \\limits_y \\int \\limits_x g(x,y)f_{X,Y}(x,y)dxdy$\n\n***Covariance***\n\n$\\mathrm{Cov}(X,Y) = \\mathrm{E}[(X-\\mathrm{E}(X))(Y - \\mathrm{E}(Y))] = \\mathrm{E}(XY) - \\mathrm{E}(X)\\mathrm{E}(Y)$\n\n***Correlation***\n\n$\\mathrm{Corr}(X,Y) = \\rho(X,Y) = \\dfrac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}}$\n\n***Sums and Products of Moments***\n\n$\\mathrm{E}(X+Y) = \\mathrm{E}(X) + \\mathrm{E}(Y)$\n\n$\\mathrm{E}(XY) = \\mathrm{E}(X)\\mathrm{E}(Y) + \\mathrm{Cov}(X,Y)$\n\nThe above 2 equation are also true for functions $g(X)$ and $h(Y)$ of the r.v.\n\n$\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\mathrm{Cov}(X, Y)$\n\n## Intro to Copulas\n\n### Joint Distribution Functions\n\nFor ERM we need to model all the risks an org. faces and their inter-dependencies\n\nOne way to do so is with a joint distribution function for all the risk\n\n$P(X_i = x_i:i=1...N) = f_{X_1,X_2,...,X_N}(x_1, x_2,...,x_N)$\n\nCorresponding joint (cumulative) distribution functions (CDF):\n\n$P(X_i \\leq x_i:i=1...N) = F_{X_1,X_2,...,X_N}(x_1, x_2,...,x_N)$\n\n### Why Copulas Are Useful\n\nEach of the org's risk are represented as the marginal distribution in the context of joint distribution functions\n\n$f_{X_1}(x_1) = \\int \\limits_{x_2} \\dots \\int \\limits_{x_N} f_{X_1,X_2,...,X_N}(x_1,x_2,...,x_N)dx_2dx_3...dx_N$\n\nThe joint distribution will combine information from the marginal risk distribution with other information on the way in which the risks interrelate or depend on one another\n\nThe joint distribution expresses this dependence of interrelated factors on one another but it does so **implicity** (You can't immediately see the nature of the interdependence by looking at the formula for the join distribution function)\n\n**Copula** can reflect this interdependence of factors **explicity**\n\n### What is a Copula\n\nCopula ($C$):  \nExpresses a multivariate cumulative distribution functions in terms of the individual marginal cumulative distributions\n\n$P(X_i \\leq x_i ; i = 1...N) = F_{X_1,...,X_N}(x_1,...,x_N) = C_{X_1,...,X_N}\\left[F_{X_1}(x_1),...,F_{X_N}(x_N)\\right]$\n\n* Where $C[\\dots]$ is the relevant copula function\n* The joint distribution function is expressed explicitly in terms of the marginal distributions and the copula function\n\n**Key Idea**:\n\n$\\left\\{\\left\\{\\begin{array}{c}\\text{Marginal distribution of} \\\\ \\text{each risk factor} \\end{array}\\right\\} \\text{combined with {Copula}} \\right\\} = \\left\\{\\begin{array}{c}\\text{Joint distribution of} \\\\ \\text{risk factors} \\end{array}\\right\\}$\n\nCan think of copula as a CDF in many dimensions. It takes marginal probabilities and combines them so as to produce a joint probability\n\n*N*-dimension copula:\n\n$C(\\mathbf{U}) = C(u_1,u_2,...,u_N) = P(U_1 \\leq u_1,...,U_N \\leq u_N)$\n\n* $u_i = F_{X_i}(x_i)$; i.e. the letter $u$ is used to denote the values of the individual CDFs, each range from $[0,1]$\n* $C$ takes in $N$ values in the range $[0,1]$ and returns another value in the range $[0,1]$ since it's a distribution function\n\n***Key beneftis***\n\n* We can deconstruct the joint distribution of a set of variables into components (marginal + copulas)\n* We can adjust each component independently of the others\n    * e.g. if the marginal distribution change shape w/o affecting the relative order of the data values within each set of observations\n    * And in this case the copula does not change (property of invariance)\n    \n### Example\n\nGiven joint PDF: $f_{X,Y}(x,y) = 6x^2y$ for $0 < x$, $y<1$\n\nMarginal PDF:\n\n* $f_X(x) = \\int 6x^2ydy = 3x^2 \\int 2ydy = 3x^2$\n* $f_Y(y) = \\int 6x^2ydx = 2y \\int 3x^2 dx = 2y$\n\nMarginal CDF:\n\n* $F_X(x) = x^3$\n* $F_Y(y) = y^2$\n\nJoint CDF:\n\n$F_{X,Y}(x,y) = \\int \\limits_{0}^x \\int \\limits_{0}^y 6t^2 s ds dt = \\int \\limits_0^x \\left[6t^2 \\dfrac{s^2}{2} \\right]^y_0 dt = \\int \\limits_0^x 3t^2y^2 dt = \\left[3 \\dfrac{t^3}{3} y^2 \\right]^x_0 = x^3 y^2$\n\nCopula corresponding to the joint PDF\n\n$u = F_X(x) = x^3 \\Rightarrow  x = u^{\\frac{1}{3}}$\n\n$u = F_Y(y) = y^2 \\Rightarrow  x = v^{\\frac{1}{2}}$\n\n$F_{X,Y}(x,y) = x^3y^2 = uv = C_{X,Y}[u,v]$\n\nThe joint CDF can be described fully by $C_{X,Y}[u,v]$ and the marginal distributions\n\n## Copulas as Probabilities\n\nConsider how copulas relate to probability distributions and review the concept of dependence\n\n### Basic Properties of Copulas\n\n***Properties of Copulas***\n\n**Property 1**\n\nIncreasing the range of values for the variables must increase the probability of observing a combination within that range\n\n$C(u_1,u_2,...,u_N)$ is an increasing function of each input variable\n\n* e.g. $C(u_1,u_2,u_3^*,u_4) > C(u_1,u_2,u_3,u_4)$ for $u^*_3 > u_3$\n* Extension of the result for a univariate probability distribution function where:  \n$F(X^*) = P(X \\leq x^*) > P(X \\leq x) = F(x)$ if $x^* > x$\n\n**Property 2**\n\nIf we \"integrate out\" all the other variables (by setting CDFs equal to the maximum value of 1 so as to include all possible values), we will just have the marginal distribution of variable $i$\n\n$C(1,...,1,u_i,1,...,1) = u_i$ for $i=1,2,...,d$ and $u_i \\in [0,1]$\n\n**Property 3**\n\nThis property ensures that a valid probability (i.e. non-negative) is produced by the copula function for any valid combination of the parameters\n\nFor all $(a_1,...,a_N)$ and $(b_1,...,b_N)$ with $0 \\leq a_i \\leq b_i \\leq 1$:  \n$\\sum \\limits_{i_1 = 1}^2 \\sum \\limits_{i_2 = 1}^2 \\dots \\sum \\limits_{i_N = 1}^2 (-1)^{i_1+\\dots+i_N} C(u_{1i_1},...,u_{Ni_N}) \\geq 0$\n\n* $u_{j1} = a_j$ and $u_{j2} = b_j$ for $j=1,2,...,N$\n* $C$ is the distribution function for the vector of r.v. $(U_1,...,U_N)$\n\n***Example of property 3***\n\nLet $(a_1, a_2)$ and $(b_1, b_2)$ be values such that $0\\leq a_1 \\leq b_1 \\leq 1$ and $0\\leq a_2 \\leq b_2 \\leq 1$\n\nThen:\n\n$\\begin{align}\n  & \\sum \\limits_{i_1 = 1}^2 \\sum \\limits_{i_2 = 1}^2 (-1)^{i_1 + i_2}C(u_{1i_1},u_{2i_2}) \\geq 0 \\\\\n  & \\Rightarrow \\sum \\limits_{i_1 = 1}^2 \\left((-1)^{i_1 +1}C(u_{1i_1},u_{21}) + (-1)^{i_1 +2} C(u_{1i_1},u_{22}) \\right) \\geq 0\\\\\n  & \\Rightarrow (-1)^2 C(u_{11},u_{21}) + (-1)^3 C(u_{11},u_{22}) + (-1)^3 C(u_{12},u_{21}) + (-1)^4C(u_{12},u_{22}) \\geq 0 \\\\\n  & \\Rightarrow C(u_{11},u_{21}) - C(u_{11},u_{22}) - C(u_{12},u_{21}) + C(u_{12},u_{22}) \\geq 0 \\\\\n\\end{align}$\n\nBy definition, $u_{11} = a_1$, $u_{21} = a_2$, $u_{12} = b_1$ and $u_{22} = b_2$ so this requires that:\n\n$C(a_1,a_2) - C(a_1,b_2) - C(b_1,a_2) + C(b_1,b_2) \\geq 0$\n\nThe inequality is equivalent to saying that the rectangle shaded diagonally downwards in the following diagram always has positive probability\n\n![](figures/figure-18.01.png)\n\nNow:  \n$\\begin{align}\n        C(b_1, b_2) - C(a_1, b_2) &= P(U_1 \\leq b_1, U_2 \\leq b_2) - P(U_1 \\leq a_1, U_2 \\leq b_2) \\\\\n        &= P(a_1 \\leq U_1 \\leq b_1, U_2 \\leq b_2)\\\\\n      \\end{align}$\n      \nAnd:  \n$\\begin{align}\n        C(b_1, a_2) - C(a_1, a_2) &= P(U_1 \\leq b_1, U_2 \\leq a_2) - P(U_1 \\leq a_1, U_2 \\leq a_2) \\\\\n        &= P(a_1 \\leq U_1 \\leq b_1, U_2 \\leq a_2)\\\\\n      \\end{align}$\n\nSubstituting into the inequality:\n\n$\\begin{align}\n  & P(a_1 \\leq U_1 \\leq b_1, U_2 \\leq b_2) - P(a_1 \\leq U_1 \\leq b_1, U_2 \\leq a_2) \\geq 0 \\\\\n  & \\Rightarrow P(a_1 \\leq U_1 \\leq b_1, a_2 \\leq U_2 \\leq b_2) \\geq 0\n  \\end{align}$\n\n### Sklar's Theorem\n\nLet $F$ be a joint distribution function with marginal CDF $F_1,...,F_N$\n\n$\\exists \\: C : \\forall \\: x_1,...,x_N \\in [-\\infty, \\infty]$\n\n$F(x_1,...,x_N) = C(F_1(x_1),...,F_N(x_N))$\n\nSklar's theorem sates that if the marginal cumulative distributions are continuous, then $C$ is unique\n\nConversely, if $C$ is a copula and $F_1,...,F_N$ are univariate CDF, then the function $F$ ($F(x_1,...,x_N) = C(F_1(x_1),...,F_N(x_N))$) is a joint cumulative distribution function with marginal CDF $F_1,...,F_N$\n\n**Definition of the copula of a distribution**\n\nIf the vector of r.v. $X$ has joint CDF $F$ with continuous marginal CDF $F_1,...,F_N$, then the copula of the distribution $F$ is the distribution function $C(F_1(x_1),...,F_N(x_N))$\n\n### Discrete Copulas\n\nEmpirical copula function describes the relationship between the marginal variables based upon their respective ranks\n\n* Such functions are examples of discrete (non-continuous) copula functions\n\nConsider a series of joint observations $(X_t, Y_t)$ for $t= 1,2,...,T$\n\n***Method 1***\n\nDefine:\n\n$\\begin{align} F(x,y) &= \\Pr(X_t \\leq x, Y_t \\leq y)\\\\\n  &= \\dfrac{1}{1+T}\\sum\\limits_{s=1}^T I(X_s \\leq x, Y_s \\leq y) \\\\\n  \\end{align}$\n  \n* $I(X_s \\leq x, Y_s \\leq y) = \\begin{cases} 1 & \\text{if }X_s \\leq x \\text{ and } Y_s \\leq y \\\\\n0 & \\text{otherwise} \\\\ \\end{cases}$\n\nIn which case:  \n$\\dfrac{1}{1+T} \\leq F(x,y) \\leq dfrac{T}{1+T}$\n\nFor sample with 99 values this would correspond to $0.01 \\leq F(x,y) \\leq 0.99$\n\n**Example**\n\n10 vectors of data $\\mathbf{X}_1,...,\\mathbf{X}_{10}$\n\n* Each contains 2 elements\n* For 3 of the 10 observations the first element takes values $\\leq 2.7$ and the second takes value $\\leq 1.4$\n\n$\\hat{F}(2.7,1,4) = \\dfrac{1}{11} \\times 3 = \\dfrac{3}{11} = 0.273$\n\n***Method 2***\n\nApply a continuity correction and define:\n\n$\\begin{align}\n  F(x,y) &= \\Pr(X_t \\leq x, Y_t \\leq y) \\\\\n  &= \\dfrac{1}{T} \\left[ \\sum\\limits_{s=1}^T I(X_s \\leq x, Y_s \\leq y) - \\dfrac{1}{2}  \\right]\n  \\end{align}$\n\nIn which case:  \n$\\dfrac{1}{2T} \\leq F(x,y) \\leq \\dfrac{T - \\frac{1}{2}}{T}$\n\n**Example**\n\n$\\hat{F}(2.7,1,4) = \\dfrac{1}{10}(3-0.5) = 0.25$\n\n### Survival Copula\n\nFor 2 variables $X$ and $Y$, **key property** of a copula:\n\n$F(x,y) = P[X\\leq x, Y\\leq y] = C[F_X(x), F_Y(y)]$\n\nFor each copula there is a corresponding **survival copula** defined by the \"opposite relationship\"\n\n$\\bar{F}(x,y) = P[X > x, Y > y] = \\bar{C}[\\bar{F}_X(x), \\bar{F}_Y(y)]$\n\n* $\\bar{F}_X(x) = 1 - F_X(x)$\n* $\\bar{F}_Y(y) = 1 - F_Y(y)$\n\nSo the survival copula expresses the joint survival probability in terms of the marginal survival probabilities\n\nRelationship between the survival copulas and the ordinary copulas\n\n$\\bar{C}(1-u,1-v) = 1 - u- v + C(u,v)$\n\nDerivation of the above:\n\n$P[X \\leq x \\text{ or } Y \\leq y] = 1 - P[X > x, Y>y]$\n\nSo: $1 - P[X > x, Y>y] = P[X\\leq x] + P[Y\\leq y] - P[X\\leq x, Y \\leq y]$\n\nIn terms of copula: $1 - \\bar{C}[\\bar{F}_X(x), \\bar{F}_Y(y)] = F_X(x) + F_Y(y) - C[F_X(x), F_Y(y)]$\n\nRearranging:$\\bar{C}[\\bar{F}_X(x), \\bar{F}_Y(y)] = 1- F_X(x) - F_Y(y) + C[F_X(x), F_Y(y)]$\n\nAnd we get the above formula\n\nThe graphic below illustrate the relationship on a copula density plot in terms of two variables $X_1$ and $X_2$\n\n![](figures/figure-18.02.png)\n\n### Copula Density Functino\n\nCopula density function describes the rate of change of the copula CDF\n\nCalculated by partial differentiation w.r.t each of the variables\n\n$c(u_1,...,u_N) = \\dfrac{\\partial^N C(u_1,...,u_n)}{\\partial u_1 ... \\partial u_N}$\n\nIf all distribution functions are continuous\n\n$c(u_1,...u_N) = \\dfrac{f(x_1,...,x_N)}{f(x_1)f(x_2)...f(x_N)}$\n\n## Concordance (or Association)\n\nIdentify at different forms of association (e.g. linear correlation with Pearson's $\\rho$) and use this categorization to select suitable potential candidate copulas from the list of established copulas (or develop bespoke copula function)\n\n### Dependence vs Concordance (or Association)\n\nConcordance does not imply that one variable directly influences the other (i.e does not imply that one is dependent upon the other)\n\nThe linear and rank correlation measures (from mod 15) indicate concordance (or association) but do not imply dependence\n\n***Axioms for a good measure of concordance***\n\nScarsini's properties of a good measure of concordance, $M_{X,Y}$, between 2 variables ($X$ and $Y$) that are linked by a specified copula $C(F_X(x),F_Y(y))$:\n\n* Completeness of domain:  \n$M_{X,Y}$ is defined for all values of $X$ and $Y$, with $X$ and $Y$ being continuous\n* Symmetry:  \n$M_{X,Y} = M_{Y,X}$\n* Coherence:  \nIf $C_{X,Y}(u_1, u_2) \\geq C_{W,Z}(u_1, u_2)$ for all $u_1, u_2 \\in [0,1]$ then $M_{X,Y} \\geq M_{W,Z}$\n* Unit range:  \n$-1\\ leq M_{X,Y} \\leq 1$ and the extremes of this range should be feasible\n* Independence:  \nIf $X$ and $Y$ are independent then $M_{X,Y} = 0$\n* Consistency:  \nIf $X = -Z$ then $M_{X,Y} = -M_{X,Y}$\n* Convergence:  \nIf $x_1, x_2,..., x_T$ and $y_1, y_2,...,y_T$ are each sequences of $T$ observations (of the r.v. $X$ and $Y$) with joint distribution function $_{T}F(x,y)$ and copula $_{T}C(F_X(x), F_Y(y))$ then if $_{T}C(F_X(x), F_Y(y))$ tends to $C(F_X(x), F_Y(y))$ as the number of observations ($T$) increases we should also have $_{T}M_{X,Y}$ tending to $M_{X,Y}$\n\nProperties above imply other properties of good measures of concordance:\n\n* If $g(X)$ and $h(Y)$ are monotonic transformations of $X$ and $Y$ then $M_{g(X),h(Y)} = M_{X,Y}$\n* If $X$ and $Y$ are co-monotonic then $M_{X,Y} = 1$\n* IF $X$ and $Y$ are counter monotonic then $M_{X,Y} = -1$\n\nSpearman's $\\rho$ and Kendall's $\\tau$ both satisfy these criteria for a good measure of concordance\n\nPearson's $\\rho$ only fulfills all the criteria when all the marginal distributions are elliptical\n\n* e.g. a perfect non-linear relationship (e.g. $Y =\\ln X$) will not result in $\\rho = 1$\n\n### Tail Dependence\n\nCopulas can be used to describe the full relationship between the marginal distributions\n\nTail dependencies are of particular interest in RM as they describe joint concentrations of risk where they might be of particular concern (at the extremes of the marginal distributions)\n\nCoefficient of the lower tail dependence:\n\n$\\begin{align}\n  _L\\lambda_{X,Y} &= \\lim \\limits_{u \\rightarrow 0^+} P\\left(X \\leq F_X^{-1}(u) \\mid Y \\leq F_Y^{-1}(u) \\right) \\\\\n  &= \\lim \\limits_{u \\rightarrow 0^+} \\dfrac{C(u,u)}{u}\\\\\n\\end{align}$\n\nCoefficient of the upper tail dependence:\n\n$\\begin{align}\n  _U\\lambda_{X,Y} &= \\lim \\limits_{u \\rightarrow 1^-} P\\left(X > F_X^{-1}(u) \\mid Y > F_Y^{-1}(u) \\right) \\\\\n  &= \\lim \\limits_{u \\rightarrow 0^+} \\dfrac{\\bar{C}(u,u)}{u}\\\\\n\\end{align}$\n\nVisually, on a copula density plot:\n\n![](figures/figure-18.03.png)\n\nLevel of tail dependences exhibited by a particular set of data will help to indicate which copula(s) might be appropriate to consider fitting\n\nBecause each copula has a specific degree of tail dependence which may be parameterized\n\n**Derivation** of the above formulas\n\n$\\begin{align}\n  _{L}\\lambda_{X,Y} &= \\lim \\limits_{u \\rightarrow 0^+} P\\left(X \\leq F_X^{-1}(u) \\mid Y \\leq F_Y^{-1}(u) \\right) \\\\\n  &= \\lim \\limits_{u \\rightarrow 0^+} \\dfrac{P\\left(X \\leq F_X^{-1}(u), Y \\leq F_Y^{-1}(u) \\right)}{P(Y \\leq F_Y^{-1}(u))} \\\\\n  &= \\lim \\limits_{u \\rightarrow 0^+} \\dfrac{P\\left(F_X(X) \\leq u, F_Y(Y) \\leq u \\right)}{P(F_Y(Y) \\leq u)} \\\\\n  &= \\lim \\limits_{u \\rightarrow 0^+} \\dfrac{C(u,u)}{C(1,u)}\\\\\n  &= \\lim \\limits_{u \\rightarrow 0^+} \\dfrac{C(u,u)}{C(1,u)}\\\\\n\\end{align}$\n\n$\\begin{align}\n  _{U}\\lambda_{X,Y} &= \\lim \\limits_{u \\rightarrow 1^-} P\\left(X > F_X^{-1}(u) \\mid Y > F_Y^{-1}(u) \\right) \\\\\n  &= \\lim \\limits_{u \\rightarrow 1^-} \\dfrac{P\\left(X > F_X^{-1}(u), Y > F_Y^{-1}(u) \\right)}{P(Y > F_Y^{-1}(u))} \\\\\n  &= \\lim \\limits_{u \\rightarrow 1^-} \\dfrac{P\\left(F_X(X) > u, F_Y(Y) > u \\right)}{P(F_Y(Y) > u)} \\\\\n  &= \\lim \\limits_{u \\rightarrow 1^-} \\dfrac{C(1-u,1-u)}{1-u}\\\\\n  &= \\lim \\limits_{u \\rightarrow 0^+} \\dfrac{\\bar{C}(u,u)}{C(u)}\\\\\n\\end{align}$\n\nThe final equality above follows as a result of replace $1-u$ with $u4 and noting that letting $u \\rightarrow 0^+$ in the limit of the new expression is the same as letting $u \\rightarrow 1^-$ in the previous expression\n\n## Three Main Types of Copulas\n\n***Fundamental Copulas***\n\nThey represent the three basic dependencies that a set of variables can display\n\n* Independence, perfect positive dependence, perfect negative dependence\n\nThey can combined to form a wider family of copula functions called the *Fréchet-Höffding* family\n\n***Explict Copulas***\n\nThey have simple closed-form expression\n\nWe will look at the general class of *Archimedean copulas* e.g. Clayton copula\n\n***Implicit Copulas***\n\nThey are based on well-known multivariate distributions, but no simple closed-form expression exists\n\ne.g. Gaussian copula (based on the normal distribution) and the *t* copula (based on *t* distribution)\n\n## Fréchet-Höffding Copulas\n\nFundamental copulas:\n\n* Independence (or product) copula\n* Co-monotonicity (or minimum) copula\n* Counter-monotonicity copula\n\n### Independence Copula\n\nOr product copula\n\n$\\begin{align}\n  _{ind}C(F_{X_1}(x_1),...,F_{X_N}(x_N)) &= _{ind}C(u_1,...,u_N) \\\\\n  &= \\prod \\limits_{i=1}^N u_N \\\\\n  &= \\prod \\limits_{i=1}^N F_{X_i}(x_i)\n\\end{align}$\n\nThe joint distribution is equal to the produce of the individual distribution functions\n\nAs the variables are independent, there is no upper or lower tail dependence  \ni.e. $_{L}\\lambda = _{U}\\lambda = 0$\n\n### Co-monotonicity Copula\n\nOr minimum copula\n\n$\\begin{align}\n  _{min}C(F_{X_1}(x_1),...,F_{X_N}(x_N)) &=_{\\mathrm{min}}C(u_1,...,u_N) \\\\\n  &= \\mathrm{min}(u_1,...,y_N) \\\\\n  &= \\mathrm{min}(F_{X_1}(x_1),...,F_{X_N}(x_N)) \\\\\n\\end{align}$\n\nCo-monotonicity copula represents the perfect positive dependence between variables\n\n* i.e. all variables can be expressed as in increasing function (a monotonic transformation) of any given one of them $\\therefore$ an increase in the value of one variable leads to an increase in the value of all the variables\n* $_{L}\\lambda = _{U}\\lambda = 1$\n\n### Counter-monotonicity Copula\n\nOr maximum copula\n\nOnly defined in a 2 dimensions:\n\n$\\begin{align}\n  _{max}C(F_{X_1}(x_1),F_{X_2}(x_2)) &=_{\\mathrm{min}}C(u,v) \\\\\n  &= \\mathrm{max}(u+v-1,0) \\\\\n  &= \\mathrm{max}(F_{X_1}(x_1) + F_{X_2}(x_2)-1,0) \\\\\n\\end{align}$\n\nCounter-monotonoicity Copula represents perfect negative dependence between two variables\n\n* i.e. a positive change in one will always  be coupled with a negative movement in the other\n\nTail dependency between the variables will only manifest itself when the variables are at opposite ends\n\n* i.e. when one is high and on is low and no special relationship when both are low or both are high\n\n$\\therefore$ $_{L}\\lambda = _{U}\\lambda = 0$\n\n### Fréchet-Höffding Bounds\n\n$\\mathrm{max}\\left\\{ \\left( \\sum \\limits_{i=1}^N u_i \\right) +1 -N, 0\\right\\} \\leq C(u_1,u_2,...,u_N) \\leq \\mathrm{min}\\{u1,u2,...,u_N\\}$\n\nIn the bivariate case the co-monotonicity and counter-monotonicity copulas represent the extremes of the possible levels of association between variables\n\n$\\therefore$ they are the upper and lower bound for all copulas (aka the Fréchet-Höffding Bounds)\n\n* Co-monotonicity = upper bound\n* Counter-monotonicity = lower bound\n* See visuals from Sweeting p. 203 204\n* Even though the counter-monotonicity copula is not defined above 2 dimensions, the Fréchet-Höffding Bounds do exists\n\n### Fréchet-Höffding Family and Mixture\n\nThe fundamental copulas are specific cases of the general Fréchet-Höffding family of copulas which are of the form\n\n$\\begin{align}\n  _{F}C(F_{X_1}(x_1),...,F_{X_N}(x_N)) = & p \\mathrm{max} \\left(\\left(\\sum \\limits_{i=1}^N F_{X_i}(x_i)\\right)-1 ,0 \\right) \\\\\n  &+ (1-p-q)\\prod \\limits{i=1}^N F_{X_i}(x_i) \\\\\n  &+ q \\mathrm{min}(F_{X_1}(x_1),...,F_{X_N}(x_N))\\\\\n\\end{align}$\n\n* $N\\geq 2$\n* $0 \\leq p \\leq 1$\n* $0 \\leq q \\leq 1$\n* $p+q\\leq1$\n* If $N>2$ then $p=0$ (as counter-mono copula only exists in 2-D)\n\nIn the bivariate case:\n\n$_{F}C(u,v) = p \\mathrm{max}(u+v-1,0) + (1-p-q)uv + q min(u,v)$\n\n**Mixture copula** is defined when\n\n* One of $p$ or $q$ = 0\n* The other is $>0$\n\n(Appendix of the CMP has more discussions on the attainable correlations)\n\n### Copula CDF Visualization\n\n![](figures/figure-18.04.png)\n![](figures/figure-18.05.png)\n![](figures/figure-18.06.png)\n\nNote how the co-monotonicity copula represents an upper bound to copulas and the counter-monotonicity copula represents the lower bound\n\n## Archimedean Copulas\n\nImportant class of explicit copulas in closed form functions\n\n### Generator Functions and Their Pseudo-Inverse\n\nA valid generator function:\n\n$\\psi: [0,1] \\rightarrow [0, \\infty]$ is a continuous and strictly decreasing function on $[0,1]$ with $\\psi(1) = 0$ and $\\psi(0) \\leq \\infty$\n\n**Pseudo-inverse*** of $\\psi$ with the domain $[0,\\infty]$\n\n$\\psi^{[-1]}(x) = \\begin{cases} \\psi^{-1}(x) & 0 \\leq x \\leq \\psi(0) \\\\ 0 & \\psi(0) < x \\leq \\infty \\\\ \\end{cases}$\n\n* This ensures that for generator functions with $\\psi(0) < \\infty$, the inverse of $\\psi$ is defined on the whole domain $[0,\\infty]$\n* If $\\psi(0) = \\infty$ then the pseudo-inverse is always equal to the \"ordinary\" inverse and the generator function is called a strict generator function\n\n### Archimedean Copulas\n\nArchimedean class copulas have form:\n\n$C(u_1,...,u_N) = \\psi^{[-1]}\\left( \\sum \\limits_{i=1}^N \\psi(u_i) \\right)$\n\nWhere $\\psi$ is a valid generator function which is additionally convex, i.e. $\\dfrac{d^2}{dx^2}\\psi(x) \\geq 0$\n\nFor bivariate: $C(u,v) = \\psi^{[-1]}(\\psi(u)+ \\psi(v))$\n\nThe definition of the pseudo-inverse function ensures that $\\sum \\limits_{i=1}^N \\psi(u_i)$, the value of the Archimedean copula will be a valid probability (?)\n\n### Characteristics of Archimedean Copulas\n\n**Advantage**\n\n* Relatively simple to use\n* Closed-form probability distributions and so avoid the need for integration\n\n**Limitations**\n\n* Small number of parameters $(< 3)$ $\\Rightarrow$ Application to heterogeneous groups of variables is limited\n\n### Fitting Archimedean Copulas\n\nKendall's $\\tau$ is a function of the parameters of the copula (for Archimedian copulas)\n\nFor **single parameter** form:\n\n* Calculate Kendall's $\\tau$ for the observations and solving to find $\\alpha$\n\nFor **multiple parameters** form:\n\n* Can't solve with just $\\tau$ due to a one-to-many relationship between $\\tau$ and the possible consistent set of parameter values\n\n(See copulas summary in appendix)\n\n### Gumbel Copula\n\n***Generator function***:\n\n$_{Gu} \\phi _{\\alpha}(u) = (-\\ln u)^{\\alpha}$ for $1 \\leq \\alpha > \\infty$\n\nGumbel copula from generator function:\n\n1. Find inverse solve for $u$:  \n$\\begin{align}\n  k &= (- \\ln(u))^{\\alpha} \\\\\n  -k^{\\frac{1}{\\alpha}} &= \\ln (u) \\\\\n  u &= \\exp \\left( -k^{\\frac{1}{\\alpha}} \\right) \\\\\n  \\psi^{-1}(u) &= \\exp \\left( -u^{\\frac{1}{\\alpha}} \\right) \\\\\n\\end{align}$\n\n2. Plug in $C(u_1,...,u_N) = \\psi^{[-1]}\\left( \\sum \\limits_{i=1}^N \\psi(u_i) \\right)$:  \n$\\begin{align}\n  _{Gu}C_{\\alpha}(u_1,...,u_N) &= \\psi^{[-1]}\\left( \\sum \\limits_{i=1}^N \\psi(u_i) \\right) \\\\\n  &= \\psi^{[-1]}\\left( \\sum \\limits_{i=1}^N (-\\ln(u_i))^{\\alpha} \\right) \\\\\n  &= \\exp \\left[ -\\left( \\sum \\limits_{i=1}^N (-\\ln(u_i))^{\\alpha} \\right)^{1/\\alpha}\\right] \\\\\n\\end{align}$\n\ne.g. **bivariate case**: $_{Gu}C_{\\alpha}(u,v) = \\exp \\left[ - \\left((-\\ln(u))^{\\alpha} + (- \\ln(v))^{\\alpha} \\right)^{1/\\alpha}\\right]$\n\n**Special cases** of the Gumbel:\n\n* $\\alpha = 1$: Independence copula\n* $\\alpha \\rightarrow \\infty$: co-monotonic (minimum) copula\n\n***Application of Gumbel***\n\nThe upper tail dependency (but no lower tail dependency) of the Gumbel makes it suitable for modeling situations where associations increase for extreme high values (but not for extreme low values)\n\n* e.g. modeling losses from a credit portfolio where losses are recorded as (+) values\n\n### Frank Copula\n\n***Generator function***:\n\n$_{Fr}\\psi_{\\alpha}(u) = - \\ln \\left( \\dfrac{e^{-\\alpha i} - 1}{e^{-\\alpha} -1}\\right)$ for $\\alpha \\in \\mathbb{R}$\n\n***Multivariate Frank Copula***:\n\n$_{Fr}C_{\\alpha}(u_1,...,u_N) = - \\dfrac{1}{\\alpha} \\ln \\left( 1 + \\dfrac{\\prod \\limits_{i=1}^N (e^{-\\alpha u_i}-1)}{(e^{-\\alpha}-1)^{N-1}} \\right)$\n\nIf $N>$ then this is only defined for $\\alpha > 0$\n\n* Else it does not satisfy the fundamental conditions required for a copula\n\n***Bivariate case***:\n\n$_{Fr}C_{\\alpha}(u,v) = - \\dfrac{1}{\\alpha} \\ln \\left( 1 + \\dfrac{ (e^{-\\alpha u}-1)(e^{-\\alpha v}-1)}{(e^{-\\alpha}-1)} \\right)$\n\n**Special cases** of the multidimensional Frank:\n\n* $N =2, \\alpha = -\\infty$: Counter-monotonicity (maximum) copula\n* $\\alpha \\rightarrow \\infty$: Co-monotonicity (minimum) copula\n\n***Application of Frank***\n\n* Application is limited by it characteristics\n* No tail dependency, symmetric form\n* Can be consider when modeling joint and last survivor annuities and exchange rate movements\n\n### Clayton Copula\n\n***Generator function***\n\n$_{Cl}\\psi_{\\alpha}(u) = \\dfrac{1}{\\alpha}(u^-{\\alpha} - 1)$\n\n***Multivariate Clayton copula***\n\n$_{Cl}C_{\\alpha}(u_1,...,u_N)=\\left( \\left\\{ \\sum \\limits_{i=1}^N u_i^{-\\alpha} \\right\\} -N +1 \\right)^{-\\frac{1}{\\alpha}}$\n\n***Bivariate case***\n\n$_{Cl}C_{\\alpha}(u,v) = \\left(u^{-\\alpha} + v^{-\\alpha}-1 \\right)^{\\frac{1}{\\alpha}$\n\n***Application of Clayton***\n\n* Absence of upper tail dependency but potential lower tail dependency (when $\\alpha >0$)\n* Suitable for modeling situations where associations increase for extreme lower values (but not for higher values)\n* e.g. modeling returns from a portfolio of investments, where negative returns are likely to occur simultaneously on a number of investments\n\n### Generalized Clayton Copula\n\n***Generator function***\n\n$_{GC}\\psi_{\\alpha,\\beta}(u) = \\dfrac{1}{\\alpha^{\\beta}}(u^{-\\alpha}-1)^{\\beta}$ for $\\alpha \\geq 0$ $\\beta \\geq 1$\n\n***Multivariate generalized Clayton***\n\n$_{GC}C_{\\alpha,\\beta}(u_1,...,u_N) = \\left( \\left\\{ \\sum \\limits_{i=1}^N \\left[ (u_i^{-\\alpha} - 1)^{\\beta} \\right] \\right}^{\\frac{1}{\\beta}} \\right)^{-\\frac{1}{\\alpha}}$\n\n***Bivariate case***\n\n$_{GC}C_{\\alpha,\\beta}(u,v) = \\left( \\left\\{ (u^{-\\alpha} - 1)^{\\beta} + (v^{-\\alpha} - 1)^{\\beta} \\right}^{\\frac{1}{\\beta}} \\right)^{-\\frac{1}{\\alpha}}$\n\n***Special case***\n\n* $\\beta = 1$: Standard Clayton copula\n\n***Application of generalized Clayton***\n\n* Has both upper and lower tail dependencies\n* Can be adjusted independently using the two parameters of the copula function\n\n***\n\nAppendix provides precise information on the dependence measure of the Archimedean copulas\n\nImportant to note that the degree of tail dependency (where it exists) is a function of the parameter(s) of the copula\n\n![](figures/figure-18.07.png)\n![](figures/figure-18.08.png)\n![](figures/figure-18.09.png)\n\nDifficult to see the differences between the example distribution functions above\n\nDensity functions below are more helpful\n\n![](figures/figure-18.10.png)\n![](figures/figure-18.11.png)\n![](figures/figure-18.12.png)\n\nNote that the level of tail dependency is determined by a parameter $\\alpha$\n\n## Implicit Copulas\n\n### Normal Copula\n\nLet $\\mathbf{X}$ be a vector of $N$ standard normal r.v. $\\therefore$ $\\mathbf{X}$ has multivariate normal distribution $N(\\mathbf{0,R})$, where $\\mathbf{R}$ is the correlation matrix of the individual r.v.\n\n* Applying the Gaussian copula to normal marginal distributions will results in a multivariate normal distribution\n\n***Noraml Copula***\n\n$\\begin{align}\n  _{Ga}C_{\\mathbf{R}}(\\mathbf{u}) &= P(\\Phi (X_1) \\leq u_1,...,\\Phi (X_N) \\leq u_N) \\\\\n  &= P(X_1 \\leq \\Phi^{-1}(u_1),...,X_N \\leq \\Phi^{-1}(u_N)) \\\\\n  &= \\boldsymbol{\\Phi}_{\\mathbf{R}}(\\Phi^{-1}(u_1),...,\\Phi^{-1}(u_N)) \\\\\n\\end{align}$\n\n* $\\Phi$ is the distribution function of a standard normal r.v.\n* $\\boldsymbol{\\Phi}_{\\mathbf{R}}$ is the joint normal distribution function of $\\mathbf{X}$ based on the $N(N-1)/2$ correlation coefficients in $\\mathbf{R}$\n\nIn general a correlation matrix will be\n\n* A symmetric matrix  \n(since $\\mathrm{Corr}(X_j, X_k) = \\mathrm{Corr}(X_k, X_j)$ for all $j$ and $k$)\n* With 1s on the leading diagonal  \n(since $\\mathrm{Corr}(X_j, X_j)$ for all $j$)\n* $\\therefore$ $N$ dimension matrix has $N(N-1)/2$ elements\n\n***Special cases***\n\n$\\mathbf{R} = \\mathbf{I}_N$ ($N$-dimensional identity matrix)\n\n* Becomes the independence copula (since no linear correlation between the variables)\n\n$\\mathbf{R}$ is a matrix consisting entirely of 1s\n\n* Becomes the minimum (co-monotonicity) copula (since there is a perfect linear relationship)\n\n***Bivariate case***\n\n$_{Ga}C_{\\rho}(u,v) = \\Phi_{\\rho}(\\Phi^{-1}(u),\\Phi^{-1}(v))$\n\n*Special case* for the bivariate case ($N=2$) with $\\rho = -1$\n\n* Becomes the maximum (counter-monotonicity) copula (since there is a perfect negative linear correlation between the variables)\n\nIn 2-D the Gaussian copula allows the joint distribution to reflect any dependence between the variables from perfect (+) to (-) dependence depending on the $\\rho$ $\\Rightarrow$ Gaussian is a **comprehensive** copula (very versatile)\n\nGaussian copula can be described as an integral:\n\n$_{Ga}C_{\\rho}(u_1, u_2) = \\dfrac{1}{2 \\pi \\sqrt{1-\\rho^2}} \\int \\limits_{-\\infty}^{\\Phi^{-1}(u_1)} \\int \\limits_{-\\infty}^{\\Phi^{-1}(u_2)} e^{-z} ds dt$\n\n* $z = \\dfrac{1}{2(1-\\rho^2)} (s^2 + t^2 - 2 \\rho s t)$\n* Shows that in 2-D the Gaussian copula is fully determined by the correlation between the 2 variables ($\\rho$)\n\n***Nature of tail dependencies***\n\nIn 2-D: if $\\mid \\rho \\mid < 1$ then the Gaussian copula has zero tail dependencies\n\n***Disadvantage***\n\n* Lack of tail dependency\n* Defined by a single parameter\n\n### Student's t\n\n***Overcoming Normal's Disadvantage***\n\n* Has 2 parameter\n* Enables the degree of dependence at the extremes of the marginals to be controlled independently of the correlation matrix by varying the number of d.f\n    \nLet $\\mathbf{X}$ be a vector of $N$ r.v. taken from a multivariate *t* with $\\gamma$ d.f., 0 mean and correlation matrix $\\mathbf{R}$\n\n* $\\mathbf{X} \\sim t(\\gamma, \\mathbf{0}, \\mathbf{R})$\n\nThen *t*-copula:\n\n$_t C_{\\gamma, \\mathbf{R}}(\\mathbf{u}) = t_{\\gamma, \\mathbf{R}}(t_{\\gamma}^{-1}(u_1),...,t_{\\gamma}^{-1}(u_N))$\n\n* $t_{\\gamma}$ is the distribution function of a univariate *t*-random variable with $\\gamma$ d.f.\n* $t_{\\gamma, \\mathbf{R}}$ is the joint CDF of the vector $\\mathbf{X}$\n\n***Special cases***\n\nIf $\\mathbf{R} = \\mathbf{I}_N$ (*N*-dimensional identity matrix)\n\n* Does **NOT** become the independence copula\n* $\\because$ r.v. from an uncorrelated multivariate *t*-distribution are not independent\n\nIf $\\mathbf{R}$ is a matrix consisting entirely of 1s\n\n* Becomes the minimum (co-monotonicity) copula\n\n$\\gamma = \\infty$\n\n* Becomes the Gaussian copula\n\n***Bivariate case***\n\n$_t C_{\\gamma,\\rho}(u_1, u_2) = \\dfrac{1}{2 \\pi \\sqrt{1-\\rho^2}} \\int \\limits_{-\\infty}^{t_{\\gamma}^{-1}(u_1)} \\int \\limits_{-\\infty}^{t_{\\gamma}^{-1}(u_2)} \\left(1 + \\dfrac{s^2 +t^2 - 2\\rho s t}{2\\gamma(1-\\rho^2)} \\right)^{\\frac{\\gamma + 2}{2}} ds dt$\n\n***Nature of tail dependencies***\n\n* For finite values of $\\gamma$ the Student's *t* copula has both upper and lower tail dependencies\n* The smaller the value of $\\gamma$, the greater the level of association at the extremes of the marginals\n    * Increased association is present in all 4 extreme corners of the copula distribution (not just upper and lower tail dependencies)\n\n![](figures/figure-18.13.png)\n![](figures/figure-18.14.png)\n\nSimilarly, it is easier to see the difference with the density functions below\n\n![](figures/figure-18.15.png)\n![](figures/figure-18.16.png)",
    "created" : 1471991040802.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2475722001",
    "id" : "4EF894F0",
    "lastKnownWriteTime" : 1471991092,
    "last_content_update" : 1471991092182,
    "path" : "~/Git Repos/Exam ST9 2016 Notes/4. Risk Modeling/18-Copulas.Rmd",
    "project_path" : "4. Risk Modeling/18-Copulas.Rmd",
    "properties" : {
        "docOutlineVisible" : "1",
        "ignored_words" : "leq,org's,CDFs,invariance,ydy,ydx,dx,univariate,Sklar's,Scarsini's,Spearman's,ln,rightarrow,dependences,parameterized,monotonicity,mathrm,monotonoicity,bivariate,Fréchet,Höffding,Sweeting,geq,infty,Archimedean,Rightarrow,Archimedian,mathbf,boldsymbol,dfrac,sim\n"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}