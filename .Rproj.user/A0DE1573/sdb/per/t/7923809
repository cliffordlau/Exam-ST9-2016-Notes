{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Module 19: Fitting Models\"\nauthor: \"C. Lau\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\noutput: \n  html_document:\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 4\n---\n\n## Module Objective\n\nRecommend a specific choice of model based on the results of both quantitative and qualitative analysis of financial or insurance data\n\n***\n\nShort list a number of possible (candidate) models (based on certain features of the data)\n\n* E.g. tail dependence shown in the data will indicate which choices of copula\n\nThis module focus on how each candidate model might be fitted to the data set\n\n1. Select the method of fitting\n2. Evaluate the goodness of fit\n    * For the model as a whole\n    * For each of the explanatory variables\n    * Use both quantitative and qualitative methods\n3. Final model selection\n\nFirst consider the fitting of a distribution to a given data set, then go on to look at the fitting of more complex models to a data set\n\nExam note:\n\n* Key objective is to be able to recommend a specific choice of model or models for further analysis of specific problems\n* Recommendation needs to be made on the basis of a variety of quantitative measures as well as qualitative analysis\n\n## Fitting a Distribution to Data\n\n### Method of Moments\n\n#### Univariate Distribution\n\nParameterization of Univariate Distribution\n\nEstablish the parameters of a distribution empirically by looking at the sample moments and equating these to the population (or true) moments\n\nE.g. for a distribution with 3 parameters and $n$ data points $x_1,...,x_n$\n\n* Set $\\mathrm{E}(X) = \\dfrac{1}{n} \\sum x_i$; $\\mathrm{E}(X^2) = \\dfrac{1}{n} \\sum x_i^2$; $\\mathrm{E}(X^3) = \\dfrac{1}{n} \\sum x_i^3$\n* Solve the equations simultaneously to obtain the parameter values needed to specify the distribution\n\n#### Copulas\n\nParameterization of copulas\n\nGenerally this is done by using estimates of the rank correlation of the data\n\n* Set the true, underlying correlation of the copula to = these estimated correlations\n* Specific circumstances will influence the choice of whether to use Kendall's $\\tau$ or Spearman's $\\rho$\n\n***For Archimedean copulas***\n\n* Gumbel, Clayton, and Frank are all characterized by $\\alpha$ via their generators $\\therefore$ estimate the value of $\\alpha$ based on the observed data\n* See mod 18 for relationships between $\\alpha$ and $\\tau$\n* So estimate the value of $\\tau$ with the data and use the formulas to set it equal to the expression of $\\alpha$ and then solve\n* e.g. Gumbel: $\\tau = 1 - \\dfrac{1}{\\alpha} \\Rightarrow \\alpha = \\dfrac{1}{1 - \\tau}$\n\n#### Features of the method of moments\n\n***Advantages***\n\n* Generally more straightforward to use than other alternatives\n\n***Disadvantages***\n\n* Parameters not necessarily the most likely ones\n* Parameter values may be outside their acceptable ranges\n\n### Maximum Likelihood\n\n#### Features of MLE\n\n**Advantages**\n\n* Only generates parameter values that are within the acceptable ranges\n* Any bias in the parameter estimates reduces as the number of observations increases\n* Distribution of each parameter estimate tends toward the normal distribution\n\n#### Univariate Distribution\n\nParameterization of Univariate Distribution\n\n**likelihood function**:  \nExpresses the joint probability of the actual observations ($x_1,...,x_T$) occurring, given the choice of candidate distribution\n\n**Maximize** (w.r.t. each parameter) the **log likelihood function**  \n$\\ln L = \\sum \\limits_{t=1}^T \\ln f(x_t)$\n\n* $f(x)$ is the marginal PDF of $X$\n* Effectively maximizing the joint probability $L=\\prod_{t=1}^T f(x_t)$\n* Involve solving several simultaneous equations of the form $\\partial (\\ln (L)) / \\partial p_i = 0$ where $p_i (i=1,...,N)$ is one of the $N$ parameters of the candidate distribution\n\n#### N-dimensional copulas\n\nParameterization of N-dimensional copulas\n\nFor MLE we require the density function of the copula $c(u_1,...,u_N) = \\dfrac{\\partial^N C(u_1,...,u_n)}{\\partial u_1 ... \\partial u_N}$\n\n* $N$ is the number of variables (= dimensions of the copula)\n* $u_n = F(x_n)$ for $n= 1,...,N$\n\nEvaluate the log-likelihood function using the $T$ observations\n\n$\\begin{align}\n  \\ln\\left(L(\\boldsymbol{\\theta})\\right) =& \\ln \\left(\\prod_{t=1}^T c_{\\theta}(u_{1,t},  u_{2,t},...,u_{N,t})\\right) \\\\\n  =& \\sum \\limits_{t=1}^T \\ln \\left(c_{\\theta}(u_{1,t},  u_{2,t},...,u_{N,t})\\right) \\\\\n\\end{align}$\n\nMaximization gets more complex as the number of variables increase\n\n* In practice this is done using a suitable computer package and the application of numerical methods\n\n***Obtaining the copula density fucntion***\n\nRecall: If all distribution functions are continuous then:\n\n$c(u_1,...u_N) = \\dfrac{f(x_1,...,x_N)}{f(x_1)f(x_2)...f(x_N)}$\n\nAnd if these marginal density function are available in closed form then the probabilities can be expressed in terms of the unknown parameters ($\\boldsymbol{\\theta}$) and the log likelihood function maximized\n\nAlternatively the values of $u_{n,t} = F(X_{n,t})$ can be derived empirically from the observations and used to calculate the log likelihood function for the candidate copula, maximizing to determine the optimum parameter values\n\n***Gaussian copula***\n\nMLE of the sample co variance matrix:\n\n$\\hat{\\boldsymbol{\\Sigma}} = \\dfrac{1}{T} \\sum \\limits_{t=1}^T \\boldsymbol{\\Phi}_t^{-1} \\left(\\boldsymbol{\\Phi}_t^{-1}\\right)'$\n\n* $\\boldsymbol{\\Phi}_t^{-1} = \\left[ \\Phi^{-1}\\left( F \\left( x_{1,t} \\right) \\right), \\Phi^{-1}\\left( F \\left( x_{2,t} \\right) \\right),..., \\Phi^{-1}\\left( F \\left( x_{N,t} \\right) \\right) \\right]'$\n\nE.g. Derivation of the sample covariance matrix when $t=1$ and $N=2$ if the marginal distributions are $N(0,1)$\n\n$\\begin{align}\n  \\hat{\\boldsymbol{\\Sigma}} &= \\boldsymbol{\\Phi}_t^{-1} \\left(\\boldsymbol{\\Phi}_t^{-1}\\right)' \\\\\n  &= \\begin{bmatrix}\n      \\Phi^{-1}_{X_1} \\left( F_{X_1} \\left( x_{1} \\right) \\right) \\\\\n      \\Phi^{-1}_{X_2} \\left( F_{X_2} \\left( x_{2} \\right) \\right) \\\\\n    \\end{bmatrix}\n    \\begin{bmatrix}\n      \\Phi^{-1}_{X_1} \\left( F_{X_1} \\left( x_{1} \\right) \\right) &\n      \\Phi^{-1}_{X_2} \\left( F_{X_2} \\left( x_{2} \\right) \\right) \\\\\n    \\end{bmatrix} \\\\\n  &= \\begin{bmatrix}\n      \\left(\\Phi^{-1}_{X_1} \\left( F_{X_1} \\left( x_{1} \\right) \\right)\\right)^2 &\n      \\Phi^{-1}_{X_1} \\left( F_{X_1} \\left( x_{1} \\right) \\right)\n      \\Phi^{-1}_{X_2} \\left( F_{X_2} \\left( x_{2} \\right) \\right) \\\\\n      \\Phi^{-1}_{X_2} \\left( F_{X_2} \\left( x_{2} \\right) \\right)\n      \\Phi^{-1}_{X_1} \\left( F_{X_1} \\left( x_{1} \\right) \\right) &\n      \\left(\\Phi^{-1}_{X_2} \\left( F_{X_2} \\left( x_{2} \\right) \\right)\\right)^2 \\\\\n    \\end{bmatrix} \\\\\n  & =\\begin{bmatrix}\n      x^2_1 & x_1x_2 \\\\\n      x_2x_1 & x^2_2 \\\\\n    \\end{bmatrix}\n\\end{align}$\n\n#### Choosing between candidate copulas\n\n1. Determined optimum parameter values for each candidate copula\n2. Compare the values of the ML functions (evaluating using the observations and the relevant optimum parameters) to select the optimal overall model\n\n## Fitting a Model to Data\n\n### Least Squares Regression\n\nModel the r.v. $Y_t$ (for $t=1,2,...,T$) with $N$ independent explanatory variables $X_{t,n}$ ($n=1,2,...,N$)\n\n$Y_t = \\beta_1 X_{t,1} + \\beta_2 X_{t,2} + \\dots + \\beta_N X_{t,N} + \\epsilon_t\n\n* $\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n* $\\epsilon_t$ quantifies the degree to which the variables $X_{t,n}$ fail to fully explain the dependent variable $Y_t$\n* $\\beta_1$ can be a constant if we set $X_{t,1}$ to be a fixed amount for all $t$\n\n#### Ordinary Least Squares\n\nFor OLS the $\\beta_n$s are selected to minimize the SSE:  \n$\\boldsymbol{\\epsilon}'\\boldsymbol{\\epsilon} = \\epsilon^2_1 + \\epsilon^2_2 + \\dots + \\epsilon^2_T$\n\nClosed form solution for the minimization problem:  \n$\\mathbf{b} = \\left(\\hat{\\beta}_1, \\hat{\\beta}_2, ... ,\\hat{\\beta}_N \\right)' = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{Y}$\n\n***Assumptions***\n\n* Linear relationship between variables (variables with non-linear relationships need to be first transform before fitting)\n* Inverse of the data exists  \n(i.e. has full column rank, no column is a linear transformation or combination of any others)\n* Error terms are not correlated with each other  \n(i.e. no serial correlation exists that has not been modeled by the explanatory variables)\n* Error terms have a constant and finite variance $\\sigma^2$\n* Error terms are normally distributed (needed for the significance tests)\n\n#### Generalized Least Squares\n\nFor GLS, the variance of the error terms is not necessarily assumed to be constant and they don't have to be uncorrelated with each other\n\nLet variances and covariances of the error terms = $\\sigma^2 \\boldsymbol{\\Omega}$\n\n* $\\sigma^2$: constant\n* $\\boldsymbol{\\Omega}$: Matrix of weightings\n\nThen:\n\n* **Uncorrelated error** terms with non-constant variance can be modeled by setting: $\\boldsymbol{\\Omega} = diag(\\Omega_{1,1}, \\Omega_{2,2},...,\\Omega_{T,T})$\n    * A diagonal matrix with entries that adjust(scale) $\\sigma^2$ appropriately over time\n* Error terms with **serial correlation** but constant variance can be modeled by including correlations between observations in the off-diagonal entries of $\\boldsymbol{\\Omega}$ and setting the diagonal entries all to 1\n* Both **heteroskedastic** and **serial correlation**  \n$\\sigma^2 \\Omega = \\sigma^2\n  \\begin{bmatrix}\n    \\Omega_{1,1} & \\rho_{1,2} & \\dots & \\rho_{1,T} \\\\\n    \\rho_{2,1} & \\Omega_{2,2} & \\dots & \\rho_{2,T} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{T,1} & \\rho_{T,2} & \\dots & \\Omega_{T,T} \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\sigma^2_1 & \\sigma_{1,2} & \\dots & \\sigma_{1,T} \\\\\n    \\sigma_{2,1} & \\sigma^2_2 & \\dots & \\sigma_{2,T} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{T,1} & \\sigma_{T,2} & \\dots & \\sigma^2_T \\\\\n  \\end{bmatrix}$\n  \nClosed form solution of minimizing the SSE:  \n$\\mathbf{b} = (b_1,b_2,...,b_N)' = (\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{Y}$\n\n***Test of the overall fit***\n\nCoefficients of determination[^Cod] ($R^2$) can be used to determine the overall fit\n\n* Higher values = better fit\n* Use adjusted coefficient of determination[^AdjCod] ($R_{\\alpha}^2$), which does not automatically increase just by adding extra parameters\n\nCan use F-test[^Ftest] to test the overall regression results (if the error terms are normally distributed)\n\n* $H_0$ is that all regression coefficients are zero\n\n[^Cod]: $R^2 = \\dfrac{SSR}{SST} = 1 - \\dfrac{SSE}{SST} \\in [0,1]$\n    \n    Where:\n\n    $\\begin{array}{c,c,c,c,c,c}\n      &SST &= &SSR &+ &SSE \\\\\n      & \\sum \\limits_{t=1}^T(Y_t - \\bar{Y})^2 &= &\\sum \\limits_{t=1}^T(\\hat{Y}_t - \\bar{Y})^2 &+ &\\sum \\limits_{t=1}^T(\\underbrace{Y_t - \\hat{Y}_t}_{\\epsilon_t^2})^2 \\\\\n    \\end{array}$\n    \n    * $SST$: = total sum of squares\n    * $SSR$ = sum of squares explained by regression\n    * $SSE$ = sum of squares error (unexplained deviations)\n    * Average of the observation: $\\bar{Y} = \\dfrac{1}{T}\\sum \\limits_{t=1}^T Y_t$\n    * Predicted value: $\\hat{Y}_t = \\sum \\limits_{n=1}^N b_n x_{t,n}$\n    \n[^AdjCod]: $R_{\\alpha}^2 = 1 - \\dfrac{SSE/(T-N)}{SST/(T-1)} = 1 - \\dfrac{T-1}{T-N}(1-R^2)$\n\n[^Ftest]: $\\dfrac{SSR/(N-1)}{SSE/(T-N)} = dfrac{R^2/(N-1)}{(1-R^2)/(T-N)} \\sim F^{N-1}_{T-N}$\n\n***Test of the individual regression coefficients***\n\nTest whether each variable is significant by estimating the variance of the error terms [^SSquare]\n\nIf we assume normally distributed errors, we can use the t-test\n\n* $H_0$ is the $\\beta_n$ is 0\n* test is for the level of significant by which the coefficient ($b_n$) differs from zero\n\n[^SSquare]: $s^2 = \\dfrac{SSE}{T-N}$\n    \n    Where:\n    \n    * Deduction of the number of explanatory variables ($N$) in the denominator ensures that the estimate is unbiased\n    * $s$ is called the standard error of the regression\n    \n    Sample covariance matrix for the vector of estimates $\\mathbf{b}$ is $\\mathbf{S_b}= s^2(\\mathbf{X}'\\mathbf{X})^{-1}$\n    \n    Square root of the $n$th element of the diagonal in $\\mathbf{S}_b is $s_{b_n}$, the standard error of the estimate $b_n$\n    \n    Assuming normally distributed error term, we can use the t-test with the following statistics:  \n    $\\dfrac{b_n - \\beta_n}{s_{b_n}} \\sim t_{T-N}$\\\n    \n    Typical confidence level is 90%\n\n### Methods based on Likelihood Functions\n\n#### Likelihood ratio test\n\nSimilar when fitting distributions but iterative techniques maybe required\n\n**Nested models**:  \nIf the second model contains all the independent variables of the first plus some additional variables\n\n**Likelihood ratio test**:  \nUsed to test whether the addition of these variables results in significantly improved explanatory power\n\n* $H_0$ is that it is not the case\n* Test statistics: $LR = -2 \\ln(L_1/L_2) \\sim \\chi^2_{N_2 - N_1}$\n    * $L_1$ and $L_2$ are the values of the likelihood functions for the 2 models\n    * $N_1$ and $N_2$ are the \\# of independent variables in each model incl. the constants\n    \n#### Information Criteria (IC)\n\nUse to compare alternative models\n\nIC are not restricted to the comparison of nested models as is the case with likelihood ratio tests\n\nIC only enable the ranking of alternative models\n\n* Does not quantify the statistical significant of any differences between the alternatives\n\n**Akaike Information Criterion** (AIC)\n\n$AIC = 2N - 2 \\ln(L)$\n\n**Bayesian Information Critersion** (BIC)\n\n$BIC = N \\ln(T) - 2 \\ln(L)$\n\n$N$: \\# of independent variables in the model\n\n$T$: \\# of observations\n\nLower the value of AIC the better the fit of the model to the data\n\nBIC penalizes the introduction of another independent variable more severely and so its used tends to result in less complex models being selected compared to when using AIC\n\n### Principal Component Analysis\n\nFit data to independent parameters weighting their relative importance by the size of their eigenvalues (Mod 16)\n\n### Singular Value Decomposition\n\n**Advantage** of PCA\n\n* Readily facilitates stochastic projections\n\n**Disadvantage** of PCA\n\n* Model parameters do not necessarily have any intuitive interpretation (limited explanatory power)\n* Requires identification of the covariance matrix for the chosen $N$ independent explanatory variables\n\n**Advantage** of SVD over PCA\n\n* Based on a least squares optimization but does not require identification of the covariance matrix\n* Operates on the original data with no requirement to identify independent variables upon which to base a regression\n\n\n**Assumptions** for using SVD\n\n* SVD determine the best linear relationship between the values of a set of $N$ variables $X_{1,t},X_{2,t},...,X_{N,t}$ at each time $t$\n* If we assume the relationship continues i.e. if we have a new row of data for time $N+1$, we can use this relationship to predict the value of one or more \"missing\" future data values for these variables\n\n***Process of applying SVD*** to a set of $M$ observations of $N$ variables\n\nLet $\\mathbf{X}$ ($M \\times N$ matrix) be a set of $M$ observations of $N$ variables\n\nIf $\\mathbf{X}$ has column rank $R$ then it can be expressed as the linear combination of $R$ orthogonal matrices\n\n* Where each matrices cannot be expressed as a linear combination of each others\n\nAnd in turn, each orthogonal matrices an be broken down as produce of 2 vectors:\n\n$\\begin{bmatrix}\n  X_{1,1} & X_{1,2} & \\dots & X_{1,N} \\\\\n  X_{2,1} & X_{2,2} & \\dots & X_{2,N} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  X_{M,1} & X_{M,2} & \\dots & X_{M,N} \\\\\n\\end{bmatrix} =\n\\sum \\limits_{i=1}^R L_i\n\\begin{bmatrix}\n  U_{1,i} \\\\\n  U_{2,i} \\\\\n  \\vdots \\\\\n  U_{M,i} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  V_{1,i} & V_{2,i} \\dots V_{N,i} \\\\\n\\end{bmatrix}$\n\n**Singular values** $L_i$:\n\n* Square roots of the eigenvectors of $\\mathbf{X}\\mathbf{X}'$\n* $L_1$ is the largest with ever decreasing subsequent value down to $L_R$\n\n**Left singular vectors**: $\\mathbf{U}_i$\n\n**Right singular vectors**: $\\mathbf{V}_i$\n\nThe 3 components above are found from the original dataset using an iterative method (rather than covariance matrix as in PCA)\n\nSimilar to PCA, a large proportion of the variation in a series of data might be explained by a small number of factors so the iterative process might be terminated before all the singular vectors have been determined e.g. when $L_r$ is sufficiently small $\\Rightarrow$ We can make good predictions of future data values based on a small subset of the other variables\n\n(See more details in appendix)\n\n#### SVD Application Example\n\nLee-Carter model for mortality rates\n\nAssumes that mortality rates at all ages ($x$) are determined by: $\\ln(m_{x,t}) = \\alpha_x+ \\beta_x \\kappa_t + \\epsilon_{x,t}$\n\n* $\\alpha_x$: age specific parameter that indicates the average level of $\\ln(m_{x,t})$ over time $t$\n* $\\beta_x$: age specific parameter that characterises the sensitivity of $\\ln(m_{x,t})$ to changes in a mortality index $\\kappa_t$\n* $\\epsilon_{x,t}$: Error term the captures all remaining variations\n\nParameters on the r.h.s are unobservable (can't use OLS or PCA)\n\nUse SVD\n\n1. Set the $\\hat{\\alpha}_x$ to the average of $\\ln m_{x,t}$ over the sample period\n2. Apply SVD to the matrix of $\\left\\{ \\ln m_{x,t} - \\hat{\\alpha}_x \\right\\}$\n3. The resulting first left and right singular vectors enable estimation of $\\beta_x$ and $\\kappa_t$ respectively\n\n(CMP has more details on the matrices...)\n\n## Model Selection\n\nFor a complex model to be worth using we need to make sure the additional complexity is justified by having **significant** improvement in the ML\n\n* If choosing simply by highest log likelihood then model with largest \\# of parameters will prevail (esp. when simpler models are nested within the largest model)\n\nAIC and BIC penalize for extra parameters (while BIC is more punitive $\\Rightarrow$ less complex models)\n\nOther tests such as $chi^2$ test\n\n***Graphical diagnostic tests***\n\n* QQ plots\n* Histograms with superimposed fitted density functions\n* Empirical CDFs with superimposed fitted CDFs\n* Autocorrelation functions of time series data (ACFs)\n\nShould be able to use the above plot and states whether or not a given plot is consistent with a stated hypothesis or model\n\n* If consistent, should propose additional quantitative tests\n* If not, should be able to interpret the plot and suggest alternative models\n\nNeed to be able to recommend a specific choice of copula by applying both quantitative and qualitative analysis using different models\n\n* Quantitative: test of goodness of fit, AIC, BIC\n* Qualitative: graphical comparisons of data with candidate copulas\n\n## Model Validation\n\nModel should be fitting using training set and then fit with the testing set of comparable size\n\nType of test depends on the form of the model\n\n**Backtesting**:  \nFitting a time series model to data from one time period and then testing how well it predicts observed values from a subsequent period\n\n**Cross-sectional models**:  \nBased on dependent variables, can be fitted using one set of the data (training set) and tested using an independent data set (not necessarily from a different time period)",
    "created" : 1471989340269.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1344157500",
    "id" : "7923809",
    "lastKnownWriteTime" : 1472005399,
    "last_content_update" : 1472005399868,
    "path" : "~/Git Repos/Exam ST9 2016 Notes/4. Risk Modeling/19-Fitting-Models.Rmd",
    "project_path" : "4. Risk Modeling/19-Fitting-Models.Rmd",
    "properties" : {
        "docOutlineVisible" : "1",
        "ignored_words" : "Parameterization,Univariate,mathrm,dfrac,Spearman's,Rightarrow,ln,boldsymbol,covariance,bmatrix,mathbf,covariances,heteroskedastic,AdjCod,Ftest,SSquare,th,sim,eigenvectors,dataset,unobservable,CDFs,Autocorrelation,ACFs\n"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}