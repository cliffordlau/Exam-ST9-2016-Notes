{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Module 16: Statistical Distributions\"\nauthor: \"C. Lau\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\noutput: \n  html_document:\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 4\n---\n\n## Module Objective\n\nAnalyze univariate and multivariate financial and insurance data (incl. asset price, credit spreads and defaults, interest rates and insurance losses) using appropriate statistical methods\n\n**Recommend a specific choice of model** based on the results of both `quantitative` and `qualitative` analysis of financial or insurance data\n\n***\n\nExam notes:\n\n* For the quantitative elements, it is required to **demonstrate understanding of the ideas behind** the methodologies and **how they are implemented** rather than learning the details of the theory\n\n* Should practice the methods in this module using suitable software\n\n* Techniques introduced here to model specific risk types of risk will be covered later\n\n* Focus of the exam is on testing the **understanding of the material** and their **ability to apply the techniques** described in the Core Reading in practical situations and scenarios\n\n* Formula for any required probability distributions (incl. means, variance, generator functions and expressions of copulas), if not already given in the Formulae & Tables book will be provided in the question\n\nShould not get bogged down in the maths!\n\n***Focus on gaining an appreciation of***:\n\n* **Shapes** of the differing distributions (esp. their skewness and the fatness of their tails) so that when given observed data, you can **suggest what distribution might be appropriate to fit**\n\n* How to **check if your observed data match a given distribution** using relevant test statistics\n\n* How to **simulate random numbers from these distributions** for subsequent modeling\n\n* How observed data may be mixture of different but related distributions - hence the need to be able to **combine distributions and achieve a specified degree of correlation**\n\n## Recall: Prereq\n\n### Moments of a Distribution\n\n$\\mu_n$ is the n^th^ moment about the mean: $\\mu_n = \\mathrm{E}\\left[(X - \\mathrm{E}[X])^n\\right]$\n\nCoefficient of ***skewness*** $= \\omega = \\dfrac{\\mu_3}{\\sigma^3}$\n\nCoefficient of ***kurtosis*** $= \\kappa = \\dfrac{\\mu_4}{\\sigma^4}$\n\n* $\\kappa = 3$: **Mesokurtic**\n\n* $\\kappa > 3$: **Leptokurtic**\n\n    Slender peak, has fatter tails\n\n* $\\kappa < 3$: **Playkurtic**\n\n**Excess kurtosis** $= \\kappa -3$:  \nMeasure of kurtosis relative to a mesokurtic distribution (e.g. the normal distribution)\n\n### Gamma Function\n\n$\\Gamma(y) = \\int \\limits_0^\\infty s^{y-1}e^{-s}ds$ for $y>0$\n\n* $\\Gamma(1) = 1$\n\n* $\\Gamma(\\alpha) = (\\alpha -1)\\Gamma(\\alpha-1)$ for $\\alpha > 1$\n\n* If $\\alpha \\in \\mathbb{Z}$ then $\\Gamma(\\alpha) = (\\alpha -1)!$\n\n* $\\Gamma\\left(\\dfrac{1}{2}\\right) = \\sqrt{\\pi}$\n\n### Matrix Algebra\n\n* Transpose: $\\mathbf{A}'$\n\n* Determinant: $\\mid \\mathbf{A} \\mid$\n\n* Identity: $\\mathbf{I}$\n\n* Inverse: $\\mathbf{A}^{-1}$\n\n* Matrix of Cofactors: $F$ such that $\\mathbf{A}^{-1} = \n\\dfrac{1}{\\mid \\mathbf{A} \\mid}F$\n\n* Orthogonal matrix: If satisfying $\\mathbf{A}'\\mathbf{A} = \\mathbf{A} \\mathbf{A}' = \\mathbf{I}$\n\n* Covariance matrix: $\\boldsymbol{\\Sigma}$\n\n* Correlation matrix: $\\mathbf{R}$\n\n## Univariate Distribution\n\nWill discuss the main univariate distributions for analysis of financial time series\n\nSee also the standard distributions listed in the yellow pages of the Formulae and Tables for Examinations\n\n*Tables* contain relevant formula for distributions marked with \\#\n\n### Univariate Discrete Distributions\n\n* [Binomial^\\#^](#bin)\n\n* [Negative binomial ^\\#^](#NB)\n\n* [Poisson^\\#^](#poi)\n\n***Challenge***\n\n* **Calculations can get unwieldy** as the *number of observations becomes large* (Bin and NB)\n\n* *Solution*:\n      \n    Use **continuous approximations**\n      \n    Still important that the characteristics of the underlying distribution are understood\n\n### Univariate Continuous Distributions\n\n***Distributions with values from $-\\infty$ to $\\infty$***\n\n* [Normal^\\#^](#normal)\n\n* [Normal mixture](#normal-mix)\n\n* [Student's t^\\#^](#t-dist)\n\n* [Skewed t^\\#^](#skew-t)\n\n**Note**:\n\n* Can still be used for variables with only non-negative values if the probability of a negative value is very small  \n(e.g. mean is sufficiently positive and the variance is sufficiently low)\n\n***Distributions with only non-negative values***\n\n* [Lognormal^\\#^](#lognormal)\n\n* [Wald^\\#^](#wald)\n\n* [$\\chi^2$^\\#^](#chi)\n\n* [Gamma^\\#^ and inverse gamma](#gamma)\n\n* [Generalized inverse gamma](#gig)\n\n* [Exponential^\\#^](#exp)\n\n* [Fréchet](#frechet)\n\n* [Pareto^\\#^](#pareto)\n\n* [Generalized Pareto](#gen-pareto)\n\n***Distribution with finite range***\n\n* [Uniform^\\#^](#uniform)\n\n* [Triangular](#triangular)\n\n### Binomial{#bin}\n\n$Bin(n,p)$ = $\\sum$ of $n$ independent and identical Bernoulli ($p$) trials\n\n$X \\sim Bin(n,p)$ is the \\# of success that occur in the $n$ trials\n\n***Limiting distribution*** \n\n* Binomial distribution as $n \\rightarrow \\infty$ is the **normal distribution**\n\n### Negative Binomial{#NB}\n\n***Type 1***: \n\n* $X$ is the \\# of the trial on which the $r$^th^ success occurs, where $r \\in \\mathbb{Z}$\n\n***Type 2***:\n\n* $Y$ be the \\# of failures before the $r$^th^ success\n\n    $Y = X-r$, where $X$ is defined as above\n\n***Geometric distribution***\n\n* Special case of the **Type 1** NB distribution with $r=1$\n\n***Practical limitations*** of the Type 1 NB (also limitation of the binomial):\n\n* CDF is **laborious to calculate**\n\n* $n!$ becomes time consuming to calculate for large values of $n$\n\n**Note**:\n\n* Type 2 NB is in the *Tables* and the combinatorial factor is written in terms of the `gamma function`\n\n### Poisson{#poi}\n\nModels the **number of events** (e.g. claims) that occur in a **specified interval of time**when the events occur one after another in time in a well defined manner that presumes:\n\n* Events occur **singly**, at a **constant rate**\n\n* Numbers of events that occur in separate (i.e. non-overlapping) time intervals are **independent of one another**\n\n    i.e. The events occur randomly at a rate of $\\lambda$ per period\n\n* Such events are said to occur according to a **Poisson process**\n\n[Poisson limit theorm](https://en.wikipedia.org/wiki/Poisson_limit_theorem)\n\n* Sequence of $Bin(n,p)$, as $n \\rightarrow \\infty$ and $p \\rightarrow 0$ together such that the mean $np$ is held constant at the value $\\lambda$\n\n* Limit leads to the distribution of the Poisson variable with parameter $\\lambda$\n\n* Subbing $\\lambda = np$ into the PDF of the binomial distribution and taking the limits will produces the probability function of Poisson\n\nPoisson can be used as an **approximation to the binomial** if $p$ is small enough (e.g. with mortality rate)\n\n* Eliminates one of the practical problems with the binomial distribution but the CDF is still laborious summations\n\n### Gaussian{#normal}\n\n***Standard normal $N(0,1)$***\n\n* PDF $\\phi$ and CDF $\\Phi$\n\n* Location parameter $\\mu = 0$\n\n* Scaling parameter $\\sigma = 1$\n\n***Key features*** of the normal distribution\n\n* $f(x) > 0$ for $-\\infty < x < \\infty$\n\n* Based on **CLT**, it will approximate the distribution of a sufficiently large number of iid r.v.\n\n* It can facilitate simple analytical solutions to complex problems  \n(e.g. when it is used as an **approximation to the binomial distribution**)\n\n***Applications***:\n\n* Use for **error term** ($\\epsilon_t \\sim N(0,\\sigma)$) when modeling random walk\n\n* Standard normal is the distribution of the **test statistic** $Z = \\dfrac{X - \\mu}{\\sigma}$ \n\n    Used to determine whether the mean of the underlying population is significantly different to an assumed mean $\\mu$ when the value of $\\sigma$ is known\n\n  * Based on a **single observation** $X$\n\n* Standard normal is the distribution of the **test statistic** $Z = \\dfrac{\\bar{X} - \\mu}{\\sigma / \\sqrt{T}}$\n\n    Used to determine whether the mean of the underlying population is significantly different from $\\mu$, where $T$ is the number of observations and when the value of $\\sigma$ is known\n\n  * Based on the **sample mean** $\\bar{X}$\n  \n***Test for normality***:\n\n1. **Graphical test**\n\n    e.g. QQ plots\n\n2. **Statistical tests**\n    \n    a. Jarque-Bera\n    \n        * Calculate the skew $\\omega$ and kurtosis $\\kappa$ with no adjustment for sample bias (Use denominator of $T$)\n    \n        * $JB = \\dfrac{T}{6}\\left(\\omega^2 + \\dfrac{\\kappa}{4}\\right)$\n    \n        * Distribution of the test statistic tends to $\\chi^2_2$ as the number of observations ($T$) tends to $\\infty$\n    \n    b. Anderson-Darling\n    \n    c. Shapiro-Wilk\n    \n    d. D'Agostino\n    \n### Normal Mean-Variance Mixture{normal-mix}\n\n$X = m(W) + \\sqrt{W}\\beta Z$\n\n* $W$: Strictly positive r.v.\n\n* $Z \\sim N(0,1)$\n\n* $Z \\perp\\!\\!\\perp W$\n\n* $m(\\cdot)$: Some function\n\n* $\\beta$: Scale factor\n\n***Benefit*** (compare to just normal): **Randomness in both** the `mean` and `variance`\n    \n* For a given value of $W$, $X \\sim N\\left(m(W),W\\beta^2\\right)$\n\n* So the distribution of $X$ depend on the value of $W$\n\n* Mean is a function of $W$; Variance $\\propto$ $W$\n\n* $W$ is not fixed and can be think of as the **underlying variable that affects the mean and variance** $X$\n\n***Special cases***\n\n1. **Generalized hyperbolic distribution**:\n\n    * $m(W) = \\alpha + \\delta W$\n    \n    * $W \\sim GIG(\\beta_1, \\beta_2, \\gamma_{GIG})$\n    \n    * GIG = [Generalized inverse Gaussian](#gig)\n\n2. [**Generalized t**](#t-dist)\n\n    * $m(W) = \\alpha$\n\n    * $\\gamma / W \\sim \\chi^2_{\\gamma}$\n\n3. [**Skewed t**](#skew-t)\n\n    * $m(W) = \\mu + \\delta W$\n    \n    * $\\delta$: Skewness parameter\n    \n    * $\\gamma / W \\sim \\chi^2_{\\gamma}$\n\n### *t* (Student's, Standard and General){#t-dist}\n\n$X = \\alpha + \\beta Y$\n\n* $X \\sim$ Generalized *t* w/ $\\gamma$ degrees of freedom\n\n* $\\alpha$: Location parameter\n\n* $\\beta$: Scaling parameter\n\n* $Y \\sim$ Student's *t* or standard *t* w/ $\\gamma$ degrees of freedom\n\n**Exam note**: \n\n* Formula in the *Table* is for standard *t* ($\\alpha = 0$ and $\\beta = 1$)\n\n***Characteristics***:\n\n* CDF of the *t* can not be determined analytically\n\n    Except when $\\gamma=1$ (**Cauchy distribution**)\n\n* *t* is *leptokurtic* (fatter tail)\n\n    Makes this an important distribution for risk modeling\n\n* Kurtosis of the standard *t* is > than the normal\n\n* The degrees of freedom ($\\gamma$) is also a **shape parameter**  \n\n    i.e. $\\gamma$ determines the kurtosis\n\n***Tails*** of the *t* follow a **power law**\n\n* `Probability` of an event falling approximately in **inverse proportion** to the `size` of the event **raised to the power** of $\\gamma + 1$\n\n* For large $y$ you can ignore the 1 in the PDF, which is then $\\propto y^{-(\\gamma + 1)}$\n\n    So for the Cauchy distribution this is the inverse square of the size of the event\n\n***Standard t as normal mixture***\n\n*Define* standard *t* in terms of a **ratio** of $N(0,1)$ and $\\chi^2$\n\n$t_{\\gamma} = \\dfrac{N(0,1)}{\\sqrt{\\chi^2_{\\gamma} \\big/ \\gamma}}$\n\n* Basis of the t-test\n\nRewrite the to has the same form as a mixture distribution\n\n$t_{\\gamma} = \\underbrace{0}_{m(W)} + \\sqrt{\\underbrace{\\dfrac{\\gamma}{\\chi^2_{\\gamma}}}_{W}} \\times \\underbrace{1}_{\\beta} \\times N(0,1)$\n\n* $W \\sim$ [inverse gamma](#gamma)\n\n***Simulation***\n\n$Y = \\alpha + \\beta \\dfrac{Z}{\\sqrt{W \\big / \\gamma}}$\n\n* **Normal mixture** and has generalized *t* with parameters $\\alpha$, $\\beta$, and $\\gamma$\n\n* $Z \\sim N(0,1)$\n\n* $W \\sim \\chi^2_{\\gamma}$\n\n* $Z \\perp\\!\\!\\perp W$\n\n* $\\mathrm{VaR}(Y) \\propto 1 \\big / W$\n\n***Statistical test of a sample mean***\n\n$Z = \\dfrac{\\bar{X} - \\mu}{s \\big / \\sqrt{T}}$\n\n* $s$ is the s.d of the sample\n\n* Testing whether a sample mean ($\\bar{X}$) is statistically different from the hypothesized mean ($\\mu$) of the source population is performed using the test statistic:\n\n* Test statistic follows the standard *t* with d.f. $\\gamma = T-1$\n\n$Z = \\dfrac{\\bar{X} - \\mu}{s \\big / \\sqrt{T}} \\Bigg / \n\\sqrt{\\dfrac{(T-1)s^2 \\big/ \\sigma^2}{T-1}}$\n\n* Rewriting $Z$ from above\n\n* $\\dfrac{\\bar{X} - \\mu}{s \\big / \\sqrt{T}} \\sim N(0,1)$\n\n* $(T-1)s^2 \\big/ \\sigma^2 \\sim \\chi^2_{T-1}$\n\n* $Z$ is a **ratio** of \\{`standard normal`\\} to the \\{*square root* of a `chi-sq` *divided* by the \\# of `d.f.`\\}\n\n    As noted above, that is the standard *t*\n\n### Skewed t{#skew-t}\n\nSame parameters as the general *t* but with the **addition of a skew parameter** $\\delta$ \n\n* General *t* = skewed *t* with $\\delta = 0$\n\n***Comparison*** of $N(0,1)$, standard *t* and skewed *t*\n\n![](figures/figure-16.01.png)\n\n* $\\delta < 0$ the distribution is left skewed (longer lower tail); \n\n* $\\delta > 0$ is right skewed (longer right tail)\n\n***Simulation***\n\n$Y = \\alpha + \\beta \\dfrac{Z}{\\sqrt{W \\big / \\gamma}} + \\delta \\dfrac{1}{W \\big / \\gamma}$\n\n* (Normal mixture) skewed *t* with parameters $\\alpha$, $\\beta$, $\\gamma$ and $\\delta$\n\n* **Mean** is $m(w) = \\alpha + \\delta \\dfrac{1}{W \\big / \\gamma}$\n\n* **Variance** (from the $Z$ term) $\\propto 1\\big/W$\n\n* $Z \\sim N(0,1)$\n\n* $W \\sim \\chi^2_{\\gamma}$\n\n* $Z \\perp\\!\\!\\perp W$\n\n### Lognormal{#lognormal}\n\n$Y = \\ln X \\sim$ Normal\n\n$\\hookrightarrow$ $X$ is lognormal\n\n***Applications*** of the lognormal\n\n* Applicable to may insurance situation since it takes **only positive values**\n\n* Can be used to model **financial variables** (e.g. asset return)\n\n    *Assumptions*\n    \n    * **Log of the variable** will follow a **random walk**\n    \n    * **Drift**: $\\ln X_t = \\mu + \\ln X_{t-1} + \\epsilon_t$\n    \n    * Returns are **iid**\n\n### Wald (or Inverse Gaussian){#wald}\n\n***Wald distribution***:  \nDescribes the **time** taken for a **Brownian motion** process to **reach a given value**\n\n* **Special case** of GIG\n\n* Only takes **positive values** and have **positive skew**\n\n* PDF has a similar form to the normal distribution but with $1/x$ included in the power\n\n    $\\therefore$ It's call inverse Gaussian\n\n***Key benefits*** of the Wald is its **aggregation properties**\n\n* For $X \\sim Wald(\\alpha, \\gamma)$\n\n    $\\hookrightarrow$ $nX \\sim Wald(n\\alpha, n\\gamma)$ for $n>0$\n\n* For $X_n \\sim Wald(\\alpha w_n, \\gamma w^2_n)$ and $X_i \\perp\\!\\!\\perp X_j$\n\n    $\\hookrightarrow$ $\\sum \\limits_{n=1}^N X_n \\sim Wald \\left( \\alpha \\sum \\limits_{n=1}^N w_n , \\gamma \\left(\\sum \\limits_{n=1}^N w_n\\right)^2\\right)$\n\n### Chi-Squared{#chi}\n\n$\\chi^2_{\\gamma}$:\n\n* Sum of $\\gamma$ $X \\sim N(0,1)$\n\n* So can be simulated as such\n\n* Special case of the [gamma distribution](#gamma)\n\n***Limitations***\n\n* The link between the `mean` ($\\mu = \\gamma > 0$) and `variance` ($\\sigma^2 = 2\\gamma$) limits the application of this distribution\n\n***Chi-squared test***\n\nCan be applied where observations can each be *associated with just one* of $N$ categories\n\n* Tests *examines whether* a **set of assumed probabilities are incorrect**\n\n*Let*: \n\n* The **actual number of observations** in the *n*^th^ category be $T_n$\n\n    $T = \\sum \\limits_{n=1}^N T_n$\n\n* $p_n$ be the **assumed probability**\n\n    Such that the **expected number of observations** in category $n$ is $Tp_n$\n\n**Test statistic**: \n\n$k = \\left( \\sum \\limits_{n=1}^N X_n^2\\right) \\sim \\chi^2_{N-1}$\n\n* Where $X_n = \\dfrac{(T_n - Tp_n)}{\\sqrt{Tp_n(1-p_n)}}$\n\n* Too large a value suggest that the set of assumed probabilities is incorrect\n\n### Exponential{#exp}\n\nHas a **single scale** parameter $\\beta$\n\nDistribution provides the **expected waiting times** between the **events of a Poisson process**\n\n***Limitations to ERM*** due to its characteristics\n\n1. Monotonically decreasing nature\n\n2. Single parameter\n\n3. Low probabilities associated with extreme values\n\n### Gamma and Inverse-Gamma\n\n#### Gamma{#gamma}\n\nGamma family has **2 positive parameters** and is a versatile family\n\n* PDF can take significantly different shapes depending on the parameters\n\n***Special cases***:\n\n* $\\gamma = 1$ $\\Rightarrow$ [Exponential](#exp)\n\n* $\\beta = 2$ $\\Rightarrow$ $\\chi^2_{2\\gamma}$\n\n**Exam Notes**:\n\n* Probabilities for a gamma is not given in the *Tables* but can be obtained by turning a gamma probability into a $\\chi^2$:\n\n    $X \\sim Gamma(\\beta, \\gamma) \\equiv \\dfrac{2X}{\\beta} \\sim \\chi^2_{2\\gamma}$\n\n***Aggregation properties***:\n\n* If $X_n \\sim Gamma(\\beta, \\gamma_n)$ are all *iid*\n\n    $\\hookrightarrow$ $\\sum \\limits_{n=1}^N X_n \\sim Gamma\\left(\\beta, \\sum \\limits_{n=1}^N \\gamma_n \\right)$\n\n* If $X \\sim Gamma(\\beta, \\gamma)$\n\n    $\\hookrightarrow$ $nX \\sim Gamma(n\\beta, \\gamma)$ if $n > 0$\n\n#### Inverse-Gamma{#inv-gamma}\n\nIf $Y \\sim Gamma$ then $X = \\dfrac{1}{Y} \\sim$ Inverse-Gamma\n\n***Fitting***\n\n* **Equating** `sample` and `population moments` and solving for the distribution's parameters\n\n* Works for both gamma and inverse gamma\n\n### Generalized Inverse Gaussian (GIG){#gig}\n\nGIG offers **significant flexibility** with regard to its shape with **3 parameters**: \n\n* $\\gamma$ $\\beta_1$ and $\\beta_2$\n\n***Special limiting cases***:\n\n* $\\beta_1=0$\n\n    $\\hookrightarrow$ $Gamma(2\\beta_2, \\gamma)$\n\n* $1/\\beta_2 \\rightarrow 0$\n\n    $\\hookrightarrow$ GIG tends to $InverseGamma(\\beta_1 / 2, -\\gamma)$\n\n* $\\gamma = - \\frac{1}{2}$:\n\n    $\\hookrightarrow$ Wald\n\n### Fréchet{#frechet}\n\nOnly has a **single parameter**\n\n*Special case* of the **generalized extreme value distribution**\n\n### Pareto{#pareto}\n\n**Two parameters**\n\n***Key features***\n\n* Monotonically decreasing\n\n* Tail follows a **power law**\n\n    Shape parameter ($\\gamma$) determining the power\n\n***Simulation***\n\n$X = \\dfrac{\\beta}{U^{\\frac{1}{\\gamma}}}$\n\n* $U \\sim U(0,1)$\n\n**Side Notes**:\n\n* Corresponding CDF is $F(x) = 1 - \\left( \\dfrac{\\beta}{x}\\right)^{\\gamma}$ for $x > \\beta$, as $1 - \\left( \\dfrac{\\beta}{\\beta \\big/ u^{\\frac{1}{\\gamma}}}\\right)^{\\gamma} = u$ and by definition $F(x) \\sim U(0,1)$\n\n* Formula for Pareto CDF in Sweeting and *Table* actually corresponds to $X = \\dfrac{\\beta}{U^{\\frac{1}{\\gamma}}} - \\beta$\n\n### Generalized Pareto{#gen-pareto}\n\n***Three parameter***\n\n* **More flexible** modeling tool\n\n* Applied in **extreme value theory** (Module 20)\n\n***Note***:\n\n* Gamma in the Pareto corresponds to the gamma in the generalized form\n\n* $\\beta$ in the Pareto corresponds to $\\beta \\gamma$ in the generalized form shown in Sweeting\n\n***Relationships***:\n\n* $\\gamma = 0$: \n\n    $\\hookrightarrow$ Exponential\n    \n* $\\gamma > 0$: \n\n    $\\hookrightarrow$ Pareto\n\n### Uniform{#uniform}\n\nAssigns equal probability to all outcomes in a range $[\\beta_1, \\beta_2]$\n\nEverything is in the *Tables* where $a = \\beta_1$ and $b = \\beta_2$\n\n### Triangular{#triangular}\n\nCan be used in cases where in additional to the upper and lower values, the **most likely value is known**\n\n* Distribution has:\n\n    `lower limit` $\\beta_1$\n    \n    `mode` $\\alpha$\n    \n    `upper limit` $\\beta_2$\n\n* **Mean** is the **average of the parameter values** \n\n    $\\mu = \\frac{1}{3}(\\beta_1 + \\alpha +\\beta_2)$\n\n* Can be positively or negatively skewed\n\n## Multivariate\n\nMultivariate distributions are **simpler than copulas** (more flexible)\n\n*Appropriate if* **limited data** is available and/or a **lower level of accuracy/flexibility** is required\n\n### Multivariable Normal\n\n$\\mathbf{X} \\sim N_N(\\boldsymbol{\\alpha}, \\boldsymbol{\\Sigma})$\n\n$\\mathbf{X} = (X_1,...,X_N)'$ a column vector of r.v. has multivariate normal distribution if\n\n$\\mathbf{X} = \\boldsymbol{\\alpha} + \\mathbf{C}\\mathbf{Z}$\n\n* $\\boldsymbol{\\alpha} = (\\alpha_{X_1},...,\\alpha_{X_N})'$  \n\n    N-dimensional column vector of **location** parameters  \n    (i.e. means)\n\n* $\\mathbf{Z} = (Z_1,...,Z_k)'$  \n\n    Vector of *iid* **standard univariate normal** r.v.\n\n* $\\boldsymbol{\\Sigma}$: covariance matrix  \n\n    Containing the **scale** and **co-scale** parameters  \n    (i.e. the variances and covariances)\n\n* $\\mathbf{C}$: $N \\times k$ matrix of constants such that $\\mathbf{C}\\mathbf{C}' = \\boldsymbol{\\Sigma}$\n\n***Mean***:\n\n* $\\mathrm{E}(\\mathbf{X}) = \\hat{\\alpha}'$ since $\\mathbf{Z}$ are standard normals\n\n***Covariance***:\n\n* $\\mathrm{Cov}(\\mathbf{X}) = \\mathrm{Cov}(\\mathbf{C}\\mathbf{Z}) = \\mathbf{C} \\mathrm{Cov}(\\mathbf{Z}) \\mathbf{C}' = \\mathbf{C}\\mathbf{C}' =\\boldsymbol{\\Sigma}$\n\n* Since $Z_i \\perp\\!\\!\\perp Z_j$ so $\\mathrm{Cov}(\\mathbf{Z})$ is the $k \\times k$ identify matrix\n\n***Parameters***\n\n* Distribution is completely characterized by its `mean vector` and `covariance matrix`\n\n* Components of $\\mathbf{X}$ are mutually independent **iff** $\\boldsymbol{\\Sigma}$ is diagonal  \n(i.e. all off diagonal entries are 0)\n\n* If the location parameters are all 0 and the scale parameters are all 1 \n\n    $\\hookrightarrow$ $\\mathbf{X}$ has standard multivariate normal distribution\n\n    $\\hookrightarrow$ Correlation matrix $\\boldsymbol{\\Sigma} = \\mathbf{R}$ (Correlation matrix)\n\n***Key limitations*** for purpose of risk management\n\n* **Tails** of the univariate marginal distributions are **too thin**\n\n* **Joint tails** do *not assign enough* weight to **joint extreme outcomes**\n\n* Distribution has a **strong form of symmetry** (elliptical symmetry)\n\n***Contour plot of the bivariate normal PDF and CDF***\n\n![](figures/figure-16.02.png)\n\n![](figures/figure-16.03.png)\n\n### Testing for Multivariate Normality\n\n***Nomenclature***:\n\n* We use Greek ($\\boldsymbol{\\Sigma}$, and \\boldsymbol{\\rho}) for statistics of the distributions and use Roman letters ($\\mathbf{S}$, $\\mathbf{R}$) for sample statistics\n\n***Standard Estimators of Covariance and Correlation***\n\n$T$ observations $\\mathbf{X}_1,...\\mathbf{X}_T$ from a $d$-dimensional vector\n\n* i.e. the $t$^th^ observations $\\mathbf{X}_t = (X_{t1},X_{t2},...,X_{td})'$\n\n#### Sample mean\n\n$\\begin{align}\n    \\bar{\\mathbf{X}} &= \\dfrac{1}{T}\\sum \\limits_{t=1}^T \\mathbf{X}_t \\\\\n    &= \\left(\\dfrac{1}{T}\\sum \\limits_{t=1}^T X_{t1}, \\dfrac{1}{T}\\sum \\limits_{t=1}^T X_{t2},...,\\dfrac{1}{T}\\sum \\limits_{t=1}^T X_{td}\\right) \\\\\n    &= (\\bar{X_1}, \\bar{X_2},...,\\bar{X_d})'\\\\\n  \\end{align}$\n\n* Standard ***method of moments estimator*** of the location vector $\\boldsymbol{\\alpha}$\n  \n* $\\bar{\\mathbf{X}}$ is an unbiased estimator for $\\mathbf{X}$\n\n#### Sample covariance matrix\n\n$\\mathbf{S} = \\dfrac{1}{T} \\sum \\limits_{t=1}^T (\\mathbf{X}_t - \\bar{\\mathbf{X}})(\\mathbf{X}-\\bar{\\mathbf{X}})'$\n\n* Standard ***method of moments estimator*** of the covariance matrix $\\boldsymbol{\\sigma}$\n\n* $\\mathbf{S}$ is a biased estimator of $\\boldsymbol{\\sigma}$\n\n$\\mathbf{S}_u = \\dfrac{T\\mathbf{S}}{T - 1} = \\dfrac{1}{T - 1} \\sum \\limits_{t=1}^T ( \\mathbf{X}_t - \\mathbf{\\bar{X}})( \\mathbf{X}_t - \\mathbf{\\bar{X}})'$\n\n* $\\mathbf{S}_u$ is an unbiased estimator for $\\mathbf{X}$\n\n$i$,$j$^th^ entry of the ***sample correlation matrix*** $\\mathbf{R}$ is:\n\n* $r_{ij} = \\dfrac{s_{ij}}{\\sqrt{s_{ii}s_{jj}}}$\n\n* $s_{ij}$ is the $i$,$j$^th^ entry of the **sample covariance matrix** $\\mathbf{S}$\n\nRemember that the properties of $\\bar{\\mathbf{X}}$, $\\mathbf{S}$, and $\\mathbf{R}$ depend on the true multivariate distribution of the observations\n\n#### Mahalanobis Distance\n\n$D_t$ is the ***Mahalanobis distance*** between $\\mathbf{X}_t$ and $\\bar{\\mathbf{X}}$ at time $t$\n\n* $D^2_t = (\\mathbf{X}_t - \\bar{\\mathbf{X}})'\\mathbf{S}^{-1}(\\mathbf{X}_t - \\bar{\\mathbf{X}})$ for $t = 1,...,T$\n\n* Calculate with the standard estimators $\\bar{\\mathbf{X}}$, $\\mathbf{S}$, $\\boldsymbol{\\alpha}$, and $\\boldsymbol{\\Sigma}$\n\n* For **Large $T$**:\n\n    We would expect $D_1^2,...D_T^2$ to behave approximately like and *iid* sample from a $\\chi^2_N$ distribution (where $N$ is the dimension of the vector)\n    \n    $\\hookrightarrow$ We can construct QQ-plots using this distribution\n\n$D_{s,t}$ ***Mahalanobis angle*** between $(\\mathbf{X}_s - \\bar{\\mathbf{X}})$ and $(\\mathbf{X}_t - \\bar{\\mathbf{X}})$:\n\n* $D_{s,t} = (\\mathbf{X}_s - \\bar{\\mathbf{X}})'\\mathbf{S}^{-1}(\\mathbf{X}_t - \\bar{\\mathbf{X}})$ for $s,t = 1,...,T$\n\n***Applications***\n\n* $D_t$ and $D_{s,t}$ are used to calculate the **skewness** and **kurtosis** of the **sample** (and used in tests of multivariate normality)\n\n* **Skew-type parameter**:\n\n    $w_N = \\dfrac{1}{T^2}\\sum \\limits_{s=1}^T \\sum \\limits_{t=1}^T D_{s,t}^3$\n\n* **Mardia's skew test statistic**:\n\n    $MST = \\dfrac{T}{6}w_N \\sim \\chi^2_{N(N+1)(N+2)/6}$\n\n* **Kurtosis-type parameter**:\n\n    $k_N = \\dfrac{1}{T}\\sum\\limits_{t=1}^T D_t^4$\n\n* **Mardia's kurtosis test statistic**:\n\n    $MKT = \\dfrac{k_N - N(N+2)}{\\sqrt{8N(N+2)/T}} \\sim N(0,1)$\n\n### Generating Multivariate Normal R.V.\n\n#### Cholesky Decomposition\n\n***Method of decomposing a matrix***\n\n* Used to generate a set of correlated normal variable from a set of independent standard normal variables\n\n* **Positive definite matrices** are always **invertible**\n\n    $\\hookrightarrow$ Can be written in the form: $\\mathbf{M} = \\mathbf{C}\\mathbf{C}'$\n\n    * $\\mathbf{C}$ is some lower triangular matrix with positive diagonal entries\n    \n        $\\begin{bmatrix}\n          c_{1,1} & 0 & 0 & \\dots & 0 \\\\\n          c_{2,1} & c_{2,2} & 0 & \\dots  & 0 \\\\\n          c_{3,1} & c_{3,2} & \\ddots &   & \\vdots \\\\\n          \\vdots & \\vdots &  &  & 0 \\\\\n          c_{N,1} & c_{N,2} & \\dots & & c_{N,N} \\\\\n        \\end{bmatrix}$\n  \n    * $\\mathbf{C}\\mathbf{C}'$ is the Cholesky decomposition for $\\mathbf{M}$\n\n        $\\mathbf{C}$ is the Cholesky factor and is denoted $\\mathbf{M}^{1/2}$\n\n***\n\n***Simlulation*** using Choleksy decomposition\n\n* To generate a vector $\\mathbf{X}$ with distribution $N_N(\\boldsymbol{\\alpha}, \\boldsymbol{\\Sigma})$ where \\boldsymbol{\\Sigma} is positive definite matrix\n\n* **Step 1**\n\n    Perform a Cholesky decomposition of $\\boldsymbol{\\Sigma}$ to obtain the Cholesky factor $\\boldsymbol{\\Sigma}^{1/2}$\n\n* **Step 2**\n\n    Generate a vector $\\mathbf{Z} = (Z_1,...,Z_N)'$ of independent standard normal variables\n\n    * Using the Polar method or the Box-Muller method (Algorithms for these methods are given on P.39 of the *Tables*)\n\n* **Step 3**\n\n    Set $\\mathbf{X} = \\boldsymbol{\\mu} + \\boldsymbol{\\Sigma}^{1/2} \\mathbf{Z}$\n    \n***Example***\n\nFor $N = 2$ and $\\boldsymbol{\\Sigma} = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\\\ \\end{bmatrix}$\n\n$\\boldsymbol{\\Sigma}^{1/2} = \\begin{bmatrix} 1 & 0 \\\\ \\rho & \\sqrt{1 - \\rho^2} \\\\ \\end{bmatrix}$\n\n$\\mathbf{X} = \\boldsymbol{\\Sigma}^{1/2} \\mathbf{Z} =\n\\begin{bmatrix}\n  1 & 0 \\\\ \n  \\rho & \\sqrt{1-\\rho^2} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  Z_1 \\\\ \n  Z_2 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n  Z_1 \\\\\n  \\rho Z_1 + \\sqrt{1-\\rho^2}Z_2 \\\\\n\\end{bmatrix}$\n\n* If $\\mathbf{Z}$ is ta column vector of independent standard normal variables\n\n    $\\hookrightarrow$ $\\mathbf{X}$ is a column vector of standard normal variables with correlation coefficient $\\rho$\n\n#### Principal Component Analysis\n\nPCA (**eignevalue decomposition**) breaks down each variable's **divergence from its mean** into a *weighted average* of **independent volatility factors**\n\n* Similar to factor based or multi factor modeling techniques in Module 15\n\n***Theory of spectral decomposition***:\n\n* For any covariance matrix $\\boldsymbol{\\Sigma} \\: \\exists \\: \\mathbf{V} : \\boldsymbol{\\Lambda} = \\mathbf{V}'\\boldsymbol{\\Sigma}\\mathbf{V}$ is a diagonal matrix\n\n* **Column** vectors of $\\mathbf{V}$ are the **eigenvectors** of $\\boldsymbol{\\Sigma}$\n\n* Values of the **diagonal** of $\\boldsymbol{\\Lambda}$ are the **eigenvalues** of $\\boldsymbol{\\Sigma}$\n\n***Principal component***: Each pair of corresponding eigenvectors and eigenvalues\n\n***Example***: the second PC\n\n$\\Delta_2 = \\mathbf{V}_2' \\boldsymbol{\\Sigma} \\mathbf{V}_2 = \n\\begin{bmatrix}\n  V_{1,2} \\\\ \n  V_{2,2} \\\\\n  \\vdots \\\\\n  V_{N,2} \\\\\n\\end{bmatrix}'\n\\begin{bmatrix}\n    \\sigma_{X_1,X_1} & \\sigma_{X_1,X_2}  & \\dots & \\sigma_{X_1,X_N} \\\\\n    \\sigma_{X_2,X_1} & \\sigma_{X_2,X_2}  & \\dots  & \\sigma_{X_2,X_N} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{X_N,X_1} & \\sigma_{X_N,X_2} & \\dots& \\sigma_{X_N,X_N} \\\\\n  \\end{bmatrix}\n\\begin{bmatrix}\n  V_{1,2} \\\\ \n  V_{2,2} \\\\\n  \\vdots \\\\\n  V_{N,2} \\\\\n\\end{bmatrix}$\n\n* The principal components can be calculated **iteratively** using the **power method**\n\nDefine $\\mathbf{L} = \\begin{bmatrix}\n    \\sqrt{\\Delta_1} & 0 & 0 & \\dots & 0 \\\\\n    0 & \\sqrt{\\Delta_2} & 0 & \\dots  & 0 \\\\\n    0 & 0 & \\ddots &   & \\vdots \\\\\n    \\vdots & \\vdots &  &  & 0 \\\\\n    0 & 0 & \\dots & 0 & \\sqrt{\\Delta_N} \\\\\n  \\end{bmatrix}$ to be the diagonal matrix containing the **square roots of the eigenvalues**\n  \n***Simulation*** using principal components:\n\n* **Step 1**:\n\n    Generating a vector $\\mathbf{Z} = (Z_1,...Z_N)'$ of independent standard normal variables\n\n* **Step 2**:\n\n    $\\mathbf{X} = \\mathbf{V}\\mathbf{L}\\mathbf{Z} + \\boldsymbol{\\mu}$ is then a vector of correlated random variables\n\n    The resulting vector will be affected by the precise method chosen to generate the principal components\n\n    * e.g. $\\boldsymbol{\\mu}$, $\\mathbf{L}$, and $\\mathbf{V}$ may or may  not be derived from the sample means and sample covariance matrix\n\n***Dimension reduction***\n\n* The influence of the principal components on the result decreases e.g. the second PC has less influence than the first\n\n* In some applications this means that a large proportion of the variability can be simulated by a smaller proportion of the total number of principal components.\n\n    In this case, fewer eigenvalues and eigenvectors are used, and fewer independent r.v. need to be generated\n    \n    $\\therefore$ reducing the dimensionality of the problem\n\n### Multivariate Normal Mean-Variance Mixture\n\n*Let*:\n\n* $\\mathbf{Z}$:\n\n    Column vector the $k$ elements of which are **standard normal** r.v.\n\n* $W$:\n\n    Some **strictly positive** r.v. that is **independent** of $W$ (?)\n\n* $\\mathbf{m}(W)$:\n\n    Function of $W$ resulting in a column vector of $N$ elements\n\n* $\\mathbf{B}$:\n\n    $N \\times k$ matrix of **co-scale parameters** such that $\\mathbf{B}\\mathbf{B}'$ is in the form of a covariance matrix\n\n*Then* $\\mathbf{X}$ is said to have a multivariate normal mean-variance mixture distribution if:  \n\n$\\mathbf{X} = \\mathbf{m}(W) + \\sqrt{W}\\mathbf{B}\\mathbf{Z}$\n\n* In which case we have: $X \\mid W = w \\sim N_N\\left(\\mathbf{m}(w), w\\mathbf{B}\\mathbf{B}'\\right)$\n\n***Special cases***:\n\n* Generalized hyperbolic distribution\n    \n    * $\\mathbf{m}(W) = \\boldsymbol{\\alpha} + \\boldsymbol{\\delta}W$\n    \n        $W$ has a generalized inverse Gaussian distribution\n\n* Multivariate *t*\n\n    * $\\mathbf{m}(W) = \\boldsymbol{\\alpha}$\n    \n        $\\gamma / W$ is $\\chi^2_{\\gamma}$\n\n* Skewed *t*\n\n    * $\\mathbf{m}(W) = \\mu + \\delta W$\n    \n        (? not vector?)\n        \n        ($\\delta$ is the skewness parameter)\n    \n        $\\gamma / W$ is $\\chi^2_{\\gamma}$\n    \n### Multivariate *t*\n\n$\\mathbf{X} \\sim t_N(\\gamma. \\boldsymbol{\\alpha}, \\mathbf{B})$\n\n* An *N*-dimensional multivariate *t* is described by parameters for:\n\n    Location ($\\boldsymbol{\\alpha}$)\n\n    Scale ($\\mathbf{B}$) \n\n    Shape (degrees of freedom $\\gamma$)\n\n***Contour plot*** of the bivariate *t*, highlighting the key differences between it and the bivariate normal (from earlier)\n\n![](figures/figure-16.04.png)\n\n* The contours shown above are the same as those shown earlier for the multivariate Gaussian\n\n* In this case the increasing gaps, as we move away from the center, indicate the relatively fatter tails of the multivariate t\n\nSignificance of the ***shape parameter*** $\\gamma$\n\n* **Fatness of the tail** is determined by $\\gamma$ selected\n\n* The **smaller d.f. the fatter the tails**\n\n* As $\\gamma \\rightarrow \\infty$ the distribution tends to the multivariate normal\n\n* Multivariate *t* has a fatter tail than multivariate normal in 2 senses:\n\n    1. **Marginal distributions** have **higher probabilities associated with extreme values** compared to the normal\n\n    2. Each combination of **jointly extreme values** has **higher probabilities** than the multivariate normal\n    \n***Notes***:\n\n* Matrix of (co-)scale parameters $\\neq$ as the covariance matrix\n\n    $\\mathrm{Cov}(\\mathbf{X}) = \\boldsymbol{\\Sigma} = \\dfrac{\\gamma}{\\gamma -2} \\mathbf{B}$\n\n* The standard multivariate *t* is defined only by `scale` and `shape` parameters\n\n    Denoted $\\mathbf{X} \\sim t_{\\gamma, \\mathbf{R}}$ where $\\mathbf{R}$ is the correlation matrix\n\n#### Generating Multivariate *t* r.v\n\nSimilar to the multivariate normal and starts by generating a vector $\\mathbf{Z} = (Z_1,...,Z_N)'$ of independent standard normal variables\n\n*Let*:\n\n* $\\mathbf{B}^{\\mathbf{D}}$ be a diagonal matrix consisting of the scale parameters of the marginal *t*\n\n* $X^2_{\\gamma}$ be the value of a r.v. distributed $\\chi^2_{\\gamma}$\n\n$\\mathbf{X} = \\dfrac{1}{\\sqrt{X^2_{\\gamma} \\big / \\gamma}} \\mathbf{B}^{\\mathbf{D}} \\mathbf{Z} + \\boldsymbol{\\alpha}$\n\n* $\\mathbf{X} \\sim t_N(\\gamma, \\boldsymbol{\\alpha}, \\mathbf{B})$ \n\n* Defining multivariate *t* in terms of a normal mixture distribution\n\n### Multivariate Skewed t\n\nAn *N*-dimensional **skewed** multivariate *t* is described by parameters for:\n\n* location ($\\boldsymbol{\\alpha}$)\n\n* Scale ($\\mathbf{B}$)\n\n* Shape (degree of freedom $\\gamma$)\n\n* Skew ($\\boldsymbol{\\delta}$)\n\nAs for the **non-skewed** case, the (co-) scale matrix described the correlations between the variables, but is not equivalent to the covariance matrix\n\n* $\\mathrm{CoV}{\\mathbf{X}} = \\boldsymbol{\\Sigma} = \\dfrac{\\gamma}{\\gamma -2}\\mathbf{B} + \\boldsymbol{\\delta}\\boldsymbol{\\delta}' \\dfrac{2\\gamma ^2}{(\\gamma - 2)^2(\\gamma - 4)}$\n\nSkewed multivariate t r.v. can be ***generated*** similarly to the non skewed case:\n\n$\\mathbf{X} = \\dfrac{1}{\\sqrt{X^2_{\\gamma} \\big / \\gamma}} \\mathbf{B}^{\\mathbf{D}}\\mathbf{Z} + \\boldsymbol{\\alpha} + \\dfrac{1}{X^2_{\\gamma} \\big / \\gamma} \\boldsymbol{\\delta}$\n\n### Spherical and Elliptical\n\n***Spherical***\n\n**Multivariate spherical distribution** is one where the **marginal distributions** are:\n\n* **Identical**, **symmetric**, **uncorrelated** with each other  \n(uncorrelated, not necessarily independent)\n\n    e.g. multivariate standard normal and normal mixture\n\n***Elliptical***\n\nIf any chosen (fixed) probability can be described by an **elliptical relationship** between the variables then the distribution is said to be elliptical\n\n* In **2 dimensions**, for a given (fixed) correlation ($\\rho_{X,Y}$) the variables have an elliptical distribution if:\n\n    $x^2 + y^2 - 2\\rho_{x,y}xy = c$ where $c$ is a constant\n\n    * In other words, each constant probability contour describes an ellipse in $(X,Y)$ space\n\n    * The special case where the correlation is 0 results in a spherical distribution\n\n    e.g. multivariate normal with distinct marginal distribution\n\n* Definition can extend to **higher dimensions**\n    \n    * *N*-dimensional elliptical distribution is one where each **constant-probability contour** is an *N*-dimensional ellipse",
    "created" : 1474246344714.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "672354793",
    "id" : "9ACDD41B",
    "lastKnownWriteTime" : 1474417080,
    "last_content_update" : 1474417080991,
    "path" : "~/Git Repos/Exam ST9 2016 Notes/4. Risk Modeling/16-Statistial-Distributions.Rmd",
    "project_path" : "4. Risk Modeling/16-Statistial-Distributions.Rmd",
    "properties" : {
        "docOutlineVisible" : "1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}