{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Module 24: Assessment of Operational Risks\"\nauthor: \"C. Lau\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\noutput: \n  html_document:\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 4\n---\n\n## Module Objective\n\nDiscuss the assessment of operational, liquidity and insurance risks\n\n## Need to Assess Op-Risk\n\n***Interest in the active management*** of op-risk has been kick-started in recent times by:\n\n1. Advent of ERM\n2. Introduction of new regulatory capital requirements\n3. Increased emphasis on sophisticated quantitative models for other types of risks\n4. There is no inherent upside to op-risk\n\nReasons why a more ***formal approach is advantageous***:\n\n5. Op-risk has been the main driver behind many cases of major financial disaster\n6. Op-risk is inter-linked with credit and market risk; important to minimize the likelihood of op-risk failure during already stressed market conditions\n7. Op-risk may otherwise be treated differently in different areas of the company; Can lead to key risks being over looked and decisions being taken based on inaccurate information or an incorrect assessment of a BU's risk-adjusted return\n\n***Benefit*** of `consistent` and `effective` op-risk management (that is distinct from the general benefits arising from management of other types of risks)\n\n* **Minimize impact of reputational damage** from incident linked to op-loss\n    * These incidents are more likely to give the company the appearance of being badly managed and ill-equipped to deal with errors than other risk events\n* **Minimizes** `day-to-day losses` and **reduces the potential** for more `extreme and costly incidents`\n* Improves ability to `meet business objectives` (less time spend on crisis management)\n* **Strengthens** `overall ERM process and framework`\n\nOp-risk management is still very much a developing area but is widely accepted that all companies should be considering this issue\n\nA `comprehensive approach` should be adopted, but the focus should primarily be on the **management of op-risk** (later module) rather than attempt to (spuriously) precisely measure the risk present\n\n## Assessing Operational Risk\n\nOrganizations have only recently started gathering operational loss data\n\nBased on initial analysis of publicly available data:\n\n* Distribution is skewed to the right\n* Severities have a heavy tailed distribution\n* Losses occur randomly in time\n* Loss frequency may vary considerably over time\n\n***2 types of losses to distinguish***\n\n1. Small day-to-day mistake made in the course of business\n\n    May be modelled e.g. using statistical distributions or high-claim-frequency non-life reserving techniques\n\n2. Infrequent, large events (e.g. major frauds or failed projects)\n\n    Require methods such as extreme value techniques (fitting GPD)\n    \n***Model approach***\n\n* **Bottom up model**:  \nEstimates the operational risk capital by **starting the analysis at a low level of detail** (i.e. looking at each category of op-risk in return) and then aggregating the results\n\n* **Top down model**:  \nUse `readily available data` and `fairly simple calculations` to give a **general picture of the operational risk** of a company\n\n* Insurance companies are more likely to practice to use a high level scenario analysis technique from a top down perspective\n\n### Bottom-up Models\n\nEstimates the operational risk capital by **starting the analysis at a low level of detail** (i.e. looking at each category of op-risk in return) and then aggregating the results\n\n#### Statistical Analysis\n\nNeed model that cope well with the outer tail of the loss distribution\n\nEVT (Mod 20) may be suitable for assessing infrequent, potentially catastrophic risks\n\n* Reasonable to assume the inflation adjusted loss amount at least have a common severity distribution\n* Some suggests the GPD as one of the most useful tools to fit loss distribution in the extreme tails\n* However EVT may be appropriate but this approach is **not widely use** due to the **lack of internal data**\n\nGiven sufficient data to come up with an estimated loss distribution, then Monte Carlo might be used to estimate op-risk capital for a given CI\n\n#### Scenario Analysis\n\nUseful technique due to the potential **linkage** between `op-risk` and `other risk`\n\n***Steps for applying scenario analysis to op-risk assessment***\n\n1. Group risk exposures into broad categories\n\n    e.g. risk invovling financial fraud; system errors, etc\n    \n    Require input from a wide range of senior individuals in the organization\n    \n2. Develop plausible adverse scenario for each group of risk\n\n    Need to be plausible to determine the consequence of the risk event\n    \n    The scenario is deemed to be representative of all risk in the group\n    \n3. Calculate the consequence of the risk event occurring for each scenario\n\n    Also will involve senior staff input\n    \n    Financial consequences could include: redress paid to those affected; cost of correcting systems and records; regulatory fees and fines; opportunity costs while any changes are made; etc\n    \n    In practice the mid-point of a range of possible values is usually taken\n    \n4. Total costs calculated are taken as the financial cost of all risks represented by the chosen scenario\n\n5. Assessment of likelihood and severity made by a scenario analysis can be displayed on a risk map\n\n***Benefits** of scenario analysis\n\n* Captures opinions, concerns and experience of risk managers\n* Not rely heavily on availability/accuracy/relevance of historical data\n* Provide opportunity to identify hard to predict, high-impact events\n* Identify and improve understanding of cause and effect relationship\n* Reduce risk-reward arbitrage opportunities\n\n#### Pros and Cons\n\n***Advantage*** of bottom-up model\n\n* Give a more robust picture of company's overall risk\n\n***Limitations***\n\n1. Difficult to break down reported aggregate losses into their constituent components\n2. Little robust internal historical data, esp. for low probability and high impact events\n3. Application of external data is difficult due to difference between companies\n\n***Basel advanced measurement approach*** (AMA)\n\nUnder the Basel AMA, op-risk is assessed using internal models (stat analysis) and scenario analysis  \n(Subject to approval and continual checking by the supervisory authorities)\n\nStandard is a 1-year holding period of a 99.9% CI\n\n* Consistent with the Basel standard for credit risk analysis (mod 30)\n\nLoss from op-risk are categorized into `8 business lines` (e.g. retail banking, agency service, etc) and further into `7 loss-event types` (e.g. internal fraud, damage to physical assets etc)\n\n3 specific areas that need credible data (on probability and expected size of potential losses)\n\n* Internal data on repetitive high frequency losses over a 3-5 years period\n* External data on non-repetitive, low frequency losses\n* Suitable stress scenarios to consider\n\nOverall, statistical methods are difficult to apply due to the lack of data (as banks have only recently started such information gathering)\n\n#### Factor-based models\n\nSimpler approach (due to lack of data) that assumed losses are related to the volume of transactions and to apply a weighting to the actual or expected volume of transactions (e.g. Basel indicator and standardized approaches)\n\n***Disadvantage***: Op-risk exposure may no be proportional to business volumes (might not a good proxy)\n\n***Basel indicator approach*** (BIA)\n\nOperational risk capital ($K_{BIA}$)\n\n$K_{BIA} = \\dfrac{\\sum \\limits_{t=1}^3 \\alpha \\: max(GI_t,0)}{\\sum \\limits_{t=1}^3 I(GI_t >0)}$\n\n* $GI_t$: Gross income in the prior year $t$\n* Basel Committee suggest that $\\alpha$ should be 15%\n\n***Basel standardized approach***(SA)\n\nSimilar to BIA except that gross income is split down and attributed to each of 8 business lines with a different multiplier each\n\n$K_{SA} = \\dfrac{1}{3} \\sum \\limits_{t=1}^3 max \\left(\\sum \\limits_{j=1}^8 \\beta_j GI_{j,t}, 0\\right)$\n\n* $\\beta_j$ is between 12% and 18% depending on the business\n* $GI_{j,t}$ is the gross income for business line $j$ in the prior year $t$\n\n### Top-down Models\n\nUse `readily available data` and `fairly simple calculations` to give a **general picture of the operational risk** of a company\n\nLook at 4 models, but they all fail to capture successfully low probability high consequence risk events\n\n#### 1. Implied Capital Model\n\nOperational risk capital = total risk capital - non-op-risk capital\n\n***Advantages**:  Simple and forward looking\n\n***Limitations**\n\n* Total risk capital needs to be estimated (not easy)\n* Inter-relationship between the different types of risks are ignored\n* Does not capture cause and effect scenarios (i.e. where op-risk arises in the company and its specific impact)\n\n#### 2. Income Volatility Model\n\nLooks at income volatility as the primary factor determining capital allocation\n\nOperational risk income volatility = total income-volatility - non-op-risk income-volatility\n\nRelative ***advantage*** over method 1.\n\n* Better data availability in respect of total income volatility than for the total risk capital needed for the previous model\n\n***Limitations***\n\n* Ignores the rapid evolution of companies and industries (over time the income volatility of companies will change)  \n(i.e. not forward looking)\n* Focus on income rather than value  \n(Does not capture the \"softer\" measures of risk, such as opportunity cost and the value of reputation/brand)\n\n#### 3. Economic Pricing Model\n\nCAPM. assumes that all market information is included in a company's share price and so the impact of any publicized op-loss can be identified by looking at the movement in a company's share price and stripping out the overall market movement\n\nRelative ***advantage*** over method 2.\n\n* Includes both the aggregate effect of specific risk events and the \"softer\" issues (e.g. opportunity cost and/or damage to reputation/brand)\n\n***Limitations***\n\n* No information is provided on losses due to specific risks (only aggregate)\n* Level of op-risk capital is unaffected by any controls put in place (little motivation to improve the risk management process)\n* Tail-end risks are not accounted for thoroughly\n* Does not help anticipate incidents of operational risk as there is no consideration of individual risks in isolation\n\n#### 4. Analogue Model\n\nUse data from similar companies to derive operational risk capital\n\nUseful if there is little internal data available (but debatable how accurately one company mirrors another in terms of risk profile)\n\n## Operational Risk Assessment Process\n\nSystematic process for managing a company's op-risk\n\n* Risk policy and organization (focus of the module)\n* Risk identification and assessment (focus of the module)\n* Capital allocation and performance measurement (cover in later module)\n* Risk mitigation and control (cover in later module)\n* Risk transfer and finance (cover in later module)\n\n#### Risk Policy and Organization\n\nA comprehensive operational risk management policy should include:\n\n1. `Principles` for operational risk management\n2. Definitions and `taxonomy` for op-risk\n3. `Objectives` and `goals` of op-risk management\n4. Op-risk management `processes and tools`\n5. `Org structure`, as it applies to operational risk management\n6. `Roles and responsibilities` of different business areas involved in operational risk management\n\n#### Risk Identification and Assessment\n\nNeed a wide range of tools and techniques as it covers a broad range of issues\n\n***1. Loss incident databases***\n\nHelp a company to **learn** from previous loss incidents, analyse **trends** and support**root causes analysis** of op-losses and any mitigation strategies applied\n\n***2. Controls self-assessment***\n\nInternal analysis of the key risks and their controls and management implications\n\n***3. Risk mapping***\n\n(Discussed in Mod 13) Ranks the company's key risk exposures by severity and frequency\n\nHelps prioritize risk and ensure resources are targeted to the most important risks\n\n***4. Risk indicators and minimum acceptable performance triggers***\n\nQuantitative measures can be set up (e.g. \\# of customer complaints) and used to establish goals and levels of minimum acceptable performance (MAP)\n\nAction plans should be in place to deal with any non attainment of the MAP",
    "created" : 1472214595230.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2628368921",
    "id" : "D5E99801",
    "lastKnownWriteTime" : 1472223877,
    "last_content_update" : 1472223877954,
    "path" : "~/Git Repos/Exam ST9 2016 Notes/5. Risk Assessment/24-Assessment-of-Operational-Risks.Rmd",
    "project_path" : "5. Risk Assessment/24-Assessment-of-Operational-Risks.Rmd",
    "properties" : {
        "docOutlineVisible" : "1",
        "ignored_words" : "BU's,reputational,Severities\n"
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}