---
title: "Module 18: Copulas"
author: "C. Lau"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 4
---

## Module Objective

Desmostrate understanding of the use of copulas as part of the process of modeling multivariate risks, including recommendation of an appropriate copula

***

ERM is interested in all the risk an org faces and ways they interact with each other

Module focus on the theory and application of copulas

Use of the techniques here to model specific types of risk will be covered in part 5

## Recall: Prereq

***PDF and CDF***

For any r.v. $X$:

* PDF: $f(x) = \Pr(X=x)$
* CDF: $F(x) = \Pr(X \leq x)$

Both have a range of $[0,1]$

***Marginal PDF***

$P(X = x) = \sum \limits_y P(X=x, Y=y)$

$f_X(x) = \int \limits_y f_{X,Y}(x,y)dy$

***Conditional PDF***

$P(X=x \mid Y=y) = \dfrac{P(X=x,Y=y)}{P(Y=y)}$

$f_{X \mid Y = y}(x,y) = \dfrac{f_{X,Y}(x,y)}{f_Y(y)}$

***Expectation***

$\mathrm{E}[g(X,Y)] = \sum \limits_x \sum \limits_y g(x,y)P(X=x,Y=y)$

$\int \limits_y \int \limits_x g(x,y)f_{X,Y}(x,y)dxdy$

***Covariance***

$\mathrm{Cov}(X,Y) = \mathrm{E}[(X-\mathrm{E}(X))(Y - \mathrm{E}(Y))] = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y)$

***Correlation***

$\mathrm{Corr}(X,Y) = \rho(X,Y) = \dfrac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)\mathrm{Var}(Y)}}$

***Sums and Products of Moments***

$\mathrm{E}(X+Y) = \mathrm{E}(X) + \mathrm{E}(Y)$

$\mathrm{E}(XY) = \mathrm{E}(X)\mathrm{E}(Y) + \mathrm{Cov}(X,Y)$

The above 2 equation are also true for functions $g(X)$ and $h(Y)$ of the r.v.

$\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2\mathrm{Cov}(X, Y)$

## Intro to Copulas

### Joint Distribution Functions

For ERM we need to model all the risks an org. faces and their interdependencies

One way to do so is with a joint distribution function for all the risk

$P(X_i = x_i:i=1...N) = f_{X_1,X_2,...,X_N}(x_1, x_2,...,x_N)$

Corresponding joint (cumulative) distribution functions (CDF):

$P(X_i \leq x_i:i=1...N) = F_{X_1,X_2,...,X_N}(x_1, x_2,...,x_N)$

### Why Copulas Are Useful

Each of the org's risk are represented as the marginal distribution in the context of joint distribution functions

$f_{X_1}(x_1) = \int \limits_{x_2} \dots \int \limits_{x_N} f_{X_1,X_2,...,X_N}(x_1,x_2,...,x_N)dx_2dx_3...dx_N$

The joint distribution will combine information from the marginal risk distribution with other information on the way in which the risks interrelate or depend on one another

The joint distribution expresses this depedence of interrelated factos on one another but it does so **implicity** (You can't immediately see the nature of the interdependence by looking at the formula for the join distribution function)

**Copula** can reflect this interdependence of factors **explicity**

### What is a Copula

Copula ($C$):  
Expresses a multivariate cumulative distribution functions in terms of the individual marginal cumulative distributions

$P(X_i \leq x_i ; i = 1...N) = F_{X_1,...,X_N}(x_1,...,x_N) = C_{X_1,...,X_N}\left[F_{X_1}(x_1),...,F_{X_N}(x_N)\right]$

* Where $C[\dots]$ is the relevant copulat function
* The joint distribution function is epxressed explicitly in terms of the marginal distirubtions and the copula function

**Key Idea**:

$\left\{\left\{\begin{array}{c}\text{Marginal distribution of} \\ \text{each risk factor} \end{array}\right\} \text{combined with {Copula}} \right\} = \left\{\begin{array}{c}\text{Joint distribution of} \\ \text{risk factors} \end{array}\right\}$

Can think of copula as a CDF in many dimensions. It takes marginal probabilities and combines them so as to produce a joint probability

*N*-dimenion copula:

$C(\mathbf{U}) = C(u_1,u_2,...,u_N) = P(U_1 \leq u_1,...,U_N \leq u_N)$

* $u_i = F_{X_i}(x_i)$; i.e. the letter $u$ is used to denote the values of the individual CDFs, each range from $[0,1]$
* $C$ takes in $N$ values in the range $[0,1]$ and returns another value in the range $[0,1]$ since it's a distribution function

***Key beneftis***

* We can deconstruct the joint distribution of a set of variables into components (marginal + copulas)
* We can adjust each component independtly of the others
    * e.g. if the marginal distribution change shape w/o affecting the relative order of the data values within each set of observations
    * And in this case the copula does not change (property of invariance)
    
### Example

Given joint PDF: $f_{X,Y}(x,y) = 6x^2y$ for $0 < x$, $y<1$

Marginal PDF:

* $f_X(x) = \int 6x^2ydy = 3x^2 \int 2ydy = 3x^2$
* $f_Y(y) = \int 6x^2ydx = 2y \int 3x^2 dx = 2y$

Marginal CDF:

* $F_X(x) = x^3$
* $F_Y(y) = y^2$

Joint CDF:

$F_{X,Y}(x,y) = \int \limits_{0}^x \int \limits_{0}^y 6t^2 s ds dt = \int \limits_0^x \left[6t^2 \dfrac{s^2}{2} \right]^y_0 dt = \int \limits_0^x 3t^2y^2 dt = \left[3 \dfrac{t^3}{3} y^2 \right]^x_0 = x^3 y^2$

Copula corresponding to the joint PDF

$u = F_X(x) = x^3 \Rightarrow  x = u^{\frac{1}{3}}$

$u = F_Y(y) = y^2 \Rightarrow  x = v^{\frac{1}{2}}$

$F_{X,Y}(x,y) = x^3y^2 = uv = C_{X,Y}[u,v]$

The joint CDF can be described fully by $C_{X,Y}[u,v]$ and the marginal distributions

## Copulas as Probabilities

Consider how copulas relate to probability distributions and review the concept of dependence

### Basic Properties of Copulas

***Properties of Copulas***

**Property 1**

Increasing the range of values for the variables must increase the probability of observing a combination within that range

$C(u_1,u_2,...,u_N)$ is an increasing function of each input varialbe

* e.g. $C(u_1,u_2,u_3^*,u_4) > C(u_1,u_2,u_3,u_4)$ for $u^*_3 > u_3$
* Extension of the result for a univariate probability distribution function where:  
$F(X^*) = P(X \leq x^*) > P(X \leq x) = F(x)$ if $x^* > x$

**Property 2**

If we "intergrate out" all the other variables (by setting CDFs equal to the maximum value of 1 so as to include all possible values), we will just have the marginal distribution of variable $i$

$C(1,...,1,u_i,1,...,1) = u_i$ for $i=1,2,...,d$ and $u_i \in [0,1]$

**Property 3**

This property ensures that a valid probability (i.e. non-negative) is produced by the copula function for any valid combination of the parameters

For all $(a_1,...,a_N)$ and $(b_1,...,b_N)$ with $0 \leq a_i \leq b_i \leq 1$:  
$\sum \limits_{i_1 = 1}^2 \sum \limits_{i_2 = 1}^2 \dots \sum \limits_{i_N = 1}^2 (-1)^{i_1+\dots+i_N} C(u_{1i_1},...,u_{Ni_N}) \geq 0$

* $u_{j1} = a_j$ and $u_{j2} = b_j$ for $j=1,2,...,N$
* $C$ is the distribution function for the vector of r.v. $(U_1,...,U_N)$

***Example of property 3***

Let $(a_1, a_2)$ and $(b_1, b_2)$ be values such that $0\leq a_1 \leq b_1 \leq 1$ and $0\leq a_2 \leq b_2 \leq 1$

Then:

$\begin{align}
  & \sum \limits_{i_1 = 1}^2 \sum \limits_{i_2 = 1}^2 (-1)^{i_1 + i_2}C(u_{1i_1},u_{2i_2}) \geq 0 \\
  & \Rightarrow \sum \limits_{i_1 = 1}^2 \left((-1)^{i_1 +1}C(u_{1i_1},u_{21}) + (-1)^{i_1 +2} C(u_{1i_1},u_{22}) \right) \geq 0\\
  & \Rightarrow (-1)^2 C(u_{11},u_{21}) + (-1)^3 C(u_{11},u_{22}) + (-1)^3 C(u_{12},u_{21}) + (-1)^4C(u_{12},u_{22}) \geq 0 \\
  & \Rightarrow C(u_{11},u_{21}) - C(u_{11},u_{22}) - C(u_{12},u_{21}) + C(u_{12},u_{22}) \geq 0 \\
\end{align}$

By definition, $u_{11} = a_1$, $u_{21} = a_2$, $u_{12} = b_1$ and $u_{22} = b_2$ so this requires that:

$C(a_1,a_2) - C(a_1,b_2) - C(b_1,a_2) + C(b_1,b_2) \geq 0$

The inequality is equivalent to saying that the rectangle shaded diagonally downwards in the following diagram always has positive probability

![](figures/figure-18.01.png)

Now:  
$\begin{align}
        C(b_1, b_2) - C(a_1, b_2) &= P(U_1 \leq b_1, U_2 \leq b_2) - P(U_1 \leq a_1, U_2 \leq b_2) \\
        &= P(a_1 \leq U_1 \leq b_1, U_2 \leq b_2)\\
      \end{align}$
      
And:  
$\begin{align}
        C(b_1, a_2) - C(a_1, a_2) &= P(U_1 \leq b_1, U_2 \leq a_2) - P(U_1 \leq a_1, U_2 \leq a_2) \\
        &= P(a_1 \leq U_1 \leq b_1, U_2 \leq a_2)\\
      \end{align}$

Substituting into the inequality:

$\begin{align}
  & P(a_1 \leq U_1 \leq b_1, U_2 \leq b_2) - P(a_1 \leq U_1 \leq b_1, U_2 \leq a_2) \geq 0 \\
  & \Rightarrow P(a_1 \leq U_1 \leq b_1, a_2 \leq U_2 \leq b_2) \geq 0
  \end{align}$

### Sklar's Theorem

Let $F$ be a joint distribution function with marginal CDF $F_1,...,F_N$

$\exists \: C : \forall \: x_1,...,x_N \in [-\infty, \infty]$

$F(x_1,...,x_N) = C(F_1(x_1),...,F_N(x_N))$

Sklar's theorm sates that if the marginal cumulative distributions are continuous, then $C$ is unique

Conversely, if $C$ is a copula and $F_1,...,F_N$ are univariate CDF, then the function $F$ ($F(x_1,...,x_N) = C(F_1(x_1),...,F_N(x_N))$) is a joint cumulative distribution function with marginal CDF $F_1,...,F_N$

**Definition of the copula of a distribution**

If the vector of r.v. $X$ has joint CDF $F$ with continuous marginal CDF $F_1,...,F_N$, then the copula of the distribution $F$ is the distribution function $C(F_1(x_1),...,F_N(x_N))$

### Discrete Copulas

Empirical copula function describes the relationship between the marginal variables based upon their respective ranks

* Such functions are examples of discrete (non-continuous) copula functions

Consider a series of joint observations $(X_t, Y_t)$ for $t= 1,2,...,T$

***Method 1***

Define:

$\begin{align} F(x,y) &= \Pr(X_t \leq x, Y_t \leq y)\\
  &= \dfrac{1}{1+T}\sum\limits_{s=1}^T I(X_s \leq x, Y_s \leq y) \\
  \end{align}$
  
* $I(X_s \leq x, Y_s \leq y) = \begin{cases} 1 & \text{if }X_s \leq x \text{ and } Y_s \leq y \\
0 & \text{otherwise} \\ \end{cases}$

In which case:  
$\dfrac{1}{1+T} \leq F(x,y) \leq dfrac{T}{1+T}$

For sample with 99 values this would correspond to $0.01 \leq F(x,y) \leq 0.99$

**Example**

10 vectors of data $\mathbf{X}_1,...,\mathbf{X}_{10}$

* Each contains 2 elements
* For 3 of the 10 observations the first element takes values $\leq 2.7$ and the second takes value $\leq 1.4$

$\hat{F}(2.7,1,4) = \dfrac{1}{11} \times 3 = \dfrac{3}{11} = 0.273$

***Method 2***

Apply a continuity corretion and define:

$\begin{align}
  F(x,y) &= \Pr(X_t \leq x, Y_t \leq y) \\
  &= \dfrac{1}{T} \left[ \sum\limits_{s=1}^T I(X_s \leq x, Y_s \leq y) - \dfrac{1}{2}  \right]
  \end{align}$

In which case:  
$\dfrac{1}{2T} \leq F(x,y) \leq \dfrac{T - \frac{1}{2}}{T}$

**Example**

$\hat{F}(2.7,1,4) = \dfrac{1}{10}(3-0.5) = 0.25$

### Survival Copula

For 2 variables $X$ and $Y$, **key property** of a copula:

$F(x,y) = P[X\leq x, Y\leq y] = C[F_X(x), F_Y(y)]$

For each copula there is a corresponding **survival copula** defined by the "opposite relationship"

$\bar{F}(x,y) = P[X > x, Y > y] = \bar{C}[\bar{F}_X(x), \bar{F}_Y(y)]$

* $\bar{F}_X(x) = 1 - F_X(x)$
* $\bar{F}_Y(y) = 1 - F_Y(y)$

So the survival copula expresses the joint survival probability in terms of the marginal survival probabilities

Relationship between the survival copulas and the ordinary copulas

$\bar{C}(1-u,1-v) = 1 - u- v + C(u,v)$

Derivation of the above:

$P[X \leq x \text{ or } Y \leq y] = 1 - P[X > x, Y>y]$

So: $1 - P[X > x, Y>y] = P[X\leq x] + P[Y\leq y] - P[X\leq x, Y \leq y]$

In terms of copula: $1 - \bar{C}[\bar{F}_X(x), \bar{F}_Y(y)] = F_X(x) + F_Y(y) - C[F_X(x), F_Y(y)]$

Rearranging:$\bar{C}[\bar{F}_X(x), \bar{F}_Y(y)] = 1- F_X(x) - F_Y(y) + C[F_X(x), F_Y(y)]$

And we get the above formula

The graphic below illustrate the relationship on a copulat density plot in terms of two variables $X_1$ and $X_2$

![](figures/figure-18.02.png)

### Copula Density Functino

Copula density function describes the rate of change of the copula CDF

Calculated by partial differentiation w.r.t each of the variables

$c(u_1,...,u_N) = \dfrac{\partial C(u_1,...,u_n)}{\partial u_1 ... \partial u_N}$

If all distribution functions are continuous

$c(u_1,...u_N) = \dfrac{f(x_1,...,x_N)}{f(x_1)f(x_2)...f(x_N)}$

## Concordance (or Association)

Identify at different forms of association (e.g. linear correlation with Pearson's $\rho$) and use this categorization to select suitable potential candiate copulas from the list of established copulas (or develop bespoke copula function)

### Dependence vs Concordance (or Association)

Concordance does not implay that one variable directly influences the other (i.e does not imply that one is dependent upon the other)

The linear and rank correlation measures (from mod 15) indicate concordance (or association) but do not imply dependence

***Axioms for a good measure of concordance***

Scarsini's properties of a good measure of concordance, $M_{X,Y}$, between 2 variables ($X$ and $Y$) that are linked by a specified copula $C(F_X(x),F_Y(y))$:

* Completeness of domain:  
$M_{X,Y}$ is defined for all values of $X$ and $Y$, with $X$ and $Y$ being continuous
* Symmetry:  
$M_{X,Y} = M_{Y,X}$
* Coherence:  
If $C_{X,Y}(u_1, u_2) \geq C_{W,Z}(u_1, u_2)$ for all $u_1, u_2 \in [0,1]$ then $M_{X,Y} \geq M_{W,Z}$
* Unit range:  
$-1\ leq M_{X,Y} \leq 1$ and the extremes of this range should be feasible
* Independence:  
If $X$ and $Y$ are independent then $M_{X,Y} = 0$
* Consistency:  
If $X = -Z$ then $M_{X,Y} = -M_{X,Y}$
* Convergence:  
If $x_1, x_2,..., x_T$ and $y_1, y_2,...,y_T$ are each sequences of $T$ observations (of the r.v. $X$ and $Y$) with joint distribution function $_{T}F(x,y)$ and copula $_{T}C(F_X(x), F_Y(y))$ then if $_{T}C(F_X(x), F_Y(y))$ tends to $C(F_X(x), F_Y(y))$ as the number of observations ($T$) increases we should also have $_{T}M_{X,Y}$ tending to $M_{X,Y}$

Properties above imply other properties of good measures of concordance:

* If $g(X)$ and $h(Y)$ are monotonic transformations of $X$ and $Y$ then $M_{g(X),h(Y)} = M_{X,Y}$
* If $X$ and $Y$ are co-monotonic then $M_{X,Y} = 1$
* IF $X$ and $Y$ are counter monotonic then $M_{X,Y} = -1$

Spearman's $\rho$ and Kendall's $\tau$ both satisfy these criteria for a good measure of concordance

Pearson's $\rho$ only fulfils all the criteria when all the marginal distributions are elliptical

* e.g. a perfect non-linear relationship (e.g. $Y =\ln X$) will not result in $\rho = 1$

### Tail Dependence

Copulas can be used to describe the full relationship between the marginal distributions

Tail dependencies are of particular interest in RM as they describe joint concentrations of risk where they might be of particular concern (at the extremes of the marginal distributions)

Coefficient of the lower tail dependence:

$\begin{align}
  _L\lambda_{X,Y} &= \lim \limits_{u \rightarrow 0^+} P\left(X \leq F_X^{-1}(u) \mid Y \leq F_Y^{-1}(u) \right) \\
  &= \lim \limits_{u \rightarrow 0^+} \dfrac{C(u,u)}{u}\\
\end{align}$

Coefficient of the upper tail dependence:

$\begin{align}
  _U\lambda_{X,Y} &= \lim \limits_{u \rightarrow 1^-} P\left(X > F_X^{-1}(u) \mid Y > F_Y^{-1}(u) \right) \\
  &= \lim \limits_{u \rightarrow 0^+} \dfrac{\bar{C}(u,u)}{u}\\
\end{align}$

Visually, on a copula density plot:

![](figures/figure-18.03.png)

Level of tail dependences exhibited by a particular set of data will help to indicate which copula(s) might be appropriate to consider fitting

Because each copula has a specific degree of tail dependence which may be parameterized

**Derivation** of the above formulas

$\begin{align}
  _{L}\lambda_{X,Y} &= \lim \limits_{u \rightarrow 0^+} P\left(X \leq F_X^{-1}(u) \mid Y \leq F_Y^{-1}(u) \right) \\
  &= \lim \limits_{u \rightarrow 0^+} \dfrac{P\left(X \leq F_X^{-1}(u), Y \leq F_Y^{-1}(u) \right)}{P(Y \leq F_Y^{-1}(u))} \\
  &= \lim \limits_{u \rightarrow 0^+} \dfrac{P\left(F_X(X) \leq u, F_Y(Y) \leq u \right)}{P(F_Y(Y) \leq u)} \\
  &= \lim \limits_{u \rightarrow 0^+} \dfrac{C(u,u)}{C(1,u)}\\
  &= \lim \limits_{u \rightarrow 0^+} \dfrac{C(u,u)}{C(1,u)}\\
\end{align}$

$\begin{align}
  _{U}\lambda_{X,Y} &= \lim \limits_{u \rightarrow 1^-} P\left(X > F_X^{-1}(u) \mid Y > F_Y^{-1}(u) \right) \\
  &= \lim \limits_{u \rightarrow 1^-} \dfrac{P\left(X > F_X^{-1}(u), Y > F_Y^{-1}(u) \right)}{P(Y > F_Y^{-1}(u))} \\
  &= \lim \limits_{u \rightarrow 1^-} \dfrac{P\left(F_X(X) > u, F_Y(Y) > u \right)}{P(F_Y(Y) > u)} \\
  &= \lim \limits_{u \rightarrow 1^-} \dfrac{C(1-u,1-u)}{1-u}\\
  &= \lim \limits_{u \rightarrow 0^+} \dfrac{\bar{C}(u,u)}{C(u)}\\
\end{align}$

The final equality above follows as a result of replace $1-u$ with $u4 and noting that letting $u \rightarrow 0^+$ in the limit of the new expression is the same as letting $u \rightarrow 1^-$ in the previous expresion

## Three Main Types of Copulas

***Fundamental Copulas***

They represent the three basic dependencies that a set of variables can display

* Independence, perfect positive dependence, perfect negative dependence

They can combined to form a wider family of copula functions called the *Fréchet-Höffding* family

***Explict Copulas***

They have simple closed-form expression

We will look at the general calss of *Archimedean copulas* e.g. Clayton copula

***Implicit Copulas***

They are based on well-known multivariate distributions, but no simple closed-form expression exists

e.g. Gaussian copula (based on the normal distribution) and the *t* copula (based on *t* distribution)

