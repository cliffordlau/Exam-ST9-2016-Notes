---
title: "Module 16: Statistical Distributions"
author: "C. Lau"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 4
---

## Module Objective

Analyze univariate and multivariate financial and insurance data (incl. asste price, credit spreads and defaults, interest rates and insurance losses) using appropriate statistical methods

Reommend a specific choice of model based on the results of both quantitative and qualitative analysis of financial or insurance data

***

Exam notes:

* For the quantitative elements, it is required to demonstrate understanding of the ideas behind the methodologies and how they are implemented rather than learning the details of the theory
* Should practice the methods in this module using suitable software
* Techniques introduced here to model specific risk types of risk will be covered later
* Focus of the exam is on testing the understanding of the material and their ability to apply the techniques described in the Core Reading in practical situations and scenarios
* Formulae for any required probability distributions (incl. means, variance, generator functions and expressions of copulas), if not already given in the Formulae & Tables book will be provided in the question

Should not get bogged down in the maths!

Focus on gaining an appreciation of:

* Shapes of the differing distributions (esp. their skewness and the fatness of their tails) so that when given observed data, you can suggest what distribution might be appropriate to fit
* How to check if your observed data match a given distribution using relvant test statistics
* How to simulate random numbers from these distributions for subsequent modeling
* How observed data may be mixture of different but related distributions - hence the need to be able to combine distributions and achieve a specified degree of correlation

## Recall: Prereq

***Moments of a Distribution***

$\mu_n$ is the n^th^ moment about the mean: $\mu_n = \mathrm{E}\left[(X - \mathrm{E}[X])^n\right]$

Coefficient of skewness $= \omega = \dfrac{\mu_3}{\sigma^3}$

Coefficient of kurtosis $= \kappa = \dfrac{\mu_4}{\sigma^4}$

* $\kappa = 3$: Mesokurtic
* $\kappa > 3$: Leptokurtic
    * Slender peak, has fatter tails
* $\kappa < 3$: Playkurtic

Excess kurtosis $= \kappa -3$: measure of kurtosis relative to a mesokurtic distribution (e.g. the normal distribution)

***Gamma Function***

$\Gamma(y) = \int \limits_0^\infty s^{y-1}e^{-s}ds$ for $y>0$

* $\Gamma(1) = 1$
* $\Gamma(\alpha) = (\alpha -1)\Gamma(\alpha-1)$ for $\alpha > 1$
* If $\alpha \in \mathbb{Z}$ then $\Gamma(\alpha) = (\alpha -1)!$
* $\Gamma\left(\dfrac{1}{2}\right) = \sqrt{\pi}$

***Matrix Algebra***

* Transpose: $\mathbf{A}'$
* Determinant: $\mid \mathbf{A} \mid$
* Identity: $\mathbf{I}$
* Inverse: $\mathbf{A}^{-1}$
* Matrix of Cofactors: $F$ such that $\mathbf{A}^{-1} = \dfrac{1}{\mid \mathbf{A} \mid}F$
* Orthogonal matrix: If satisfying $\mathbf{A}'\mathbf{A} = \mathbf{A} \mathbf{A}' = \mathbf{I}$
* Covariance matrix: $\Sigma$
* Correlation matrix: $\mathbf{R}$

## Univariate Distribution

Main univariate distributions for analysis of financial time series

See also the standard distributions listed in the yellow pages of the Formulae and Tables for Examinations

Tables contain relevant formulae for distributions marked with \#

### Univariate Discrete Distributions

* Binomial^\#^
* Negative binomial ^\#^
* Poisson^\#^

Calculations with these distributions can get unwiedly as the number of observations becomes large

* Use continuous approximations
* Still important that the characteristics of the underlying distribution are understood

### Univariate Continuous Distributions

Distributions with values from $-\infty$ to $\infty$

* Normal^\#^
* Normal mixture
* Student's t^\#^
* Skewed t^\#^

We can still use the above for variables with only non-negative values if the probability of a negative value is very small (e.g. mean is sufficiently positive and the variance is sufficiently low)

Distributions with only non-negative values

* Lognormal^\#^
* Wald^\#^
* $\chi^2$^\#^
* Gamma^\#^ and inverse gamma
* Generalized inverse gamma
* Exponential^\#^
* FrÃ©chet
* Pareto^\#^
* Generalized Pareto

Distribution with finite range

* Uniform^\#^
* Triangular

### Binomial

A $Bin(n,p)$ distribution is the sum of $n$ independent and identical Bernoulli ($p$) trials

R.v. $X \sim Bin(n,p)$ is the \# of success that occur in the $n$ trials

The limiting distribution of the binomial distribution as $n \rightarrow \infty$ is the **normal distribution**

### Negative Binomial

Type 1: R.v. $X$ is the \# of the trial on which the $r$^th^ success occurs, where $r \in \mathbb{Z}$

Type 2: Let $Y$ be the \# of failures before the $r$^th^ success; $Y = X-r$, where $X$ is defined as above

Geometric distribution is a special case of the Type 1 NB distribution with $r=1$

Practical limitations of the Type 1 NB (also limitation of the binomial):

* CDF is laborious to calculate
* $n!$ becomes time consuming to calculate for large values of $n$

Type 2 NB is in the *Tables* and the combinatorial factor is written in terms of the gamma function

### Poisson

Models the number of events (e.g. cliams) that occur in a specified interval of time, when the events occur one after another in time in a well defined manner that presumes:

* Events occur singly, at a constant rate
* Numbers of events that occur in separate (i.e. non-overlapping) time intervals are independent of one another
* i.e. The events occur randomly at a rate of $\lambda$ per period
* Such events are said to occur according to a poisson process

[Poisson limit theorm](https://en.wikipedia.org/wiki/Poisson_limit_theorem)

* Sequence of $Bin(n,p)$, as $n \rightarrow \infty$ and $p \rightarrow 0$ together such that the mena $np$ is held constant at the value $\lambda$
* Limit leads to the distribution of the poisson variable with parameter $\lambda$
* Subbing $\lambda = np$ into the PDF of the binomial distribution and taking the limits will produces the probability function of poisson

Poisson can be used as an approxiamtion to the binomial if $p$ is small enough (e.g. with mortality rate)

* Eliminates one of the practical problems with the binomial distribution but the CDF is still laborious summations

### Gaussian

Standard normal

* PDF $\phi$ and CDF $\Phi$
* Location parameter $\mu = 0$
* Scaling parameter $\sigma = 1$

Key features of the normal disbribution

* $f(x) > 0$ for $-\infty < x < \infty$
* Based on CLT, it will approximate the distribution of a sufficiently large number of iid r.v.
* It can facilitate simple analytical solutions to complex problems (e.g. when it is used as an approximation to the binomial distribution)

Applications:

* Use for error term ($\epsilon_t \sim N(0,\sigma)$) when modeling random walk
* Standard normal is the distribution of the test statistic $Z = \dfrac{X - \mu}{\sigma}$ used to determine whether the mean of the underlying population is significantly different to an assumed mean $\mu$ when the value of $\sigma$ is known
  * Based on a single obervation $X$
* Standard normal is the distribution of the test statistic $Z = \dfrac{\bar{X} - \mu}{\sigma / \sqrt{T}}$ used to determine whether the mean of the underlying population is significantly different from $\mu$, where $T$ is the number of observations and when the value of $\sigma$ is known
  * Based on the sample mean $\bar{X}$
  
Test for normality:

1. Graphical test e.g. QQ plots
2. Statistical tests
    * Jarque-Bera
        * Calculate the skew $\omega$ and kurtosis $\kappa$ with no adjustment for sample bias (Use denominator of $T$)
        * $JB = \dfrac{T}{6}\left(\omega^2 + \dfrac{\kappa}{4}\right)$
        * Distribution of the test statistic tends to $\chi^2_2$ as the number of observations ($T$) tends to $\infty$
    * Anderson-Darling
    * Shapiro-Wilk
    * D'Agostino
    
### Normal Mean-Variance Mixture

$X = m(W) + \sqrt{W}\beta Z$

* $W$: Strictly positive r.v.
* $Z \sim N(0,1)$
* $Z \perp\!\!\perp W$
* $m(\cdot)$: Some function
* $\beta$: Scale factor

***Benefit*** (compare to just normal)

Randomness in both the mean and variance
    
* For a given value of $W$, $X \sim N\left(m(W),W\beta^2\right)$
* So the distribution of $X$ depend on the value of $W$
* Mean is a function of $W$; Variance $\propto$ $W$
* $W$ is not fixed and can be think of as the underlying variable that affects the mean and variance $X$

***Special cases***

Generalized hyperbolic distribution:

* $m(W) = \alpha + \delta W$
* $W \sim GIG(\beta_1, \beta_2, \gamma_{GIG})$
* GIG = Generalized inverse gaussian

Generalized t

* $m(W) = \alpha$
* $\gamma / W \sim \chi^2_{\gamma}$

Skewed t

* $m(W) = \mu + \delta W$
* $\delta$: Skewness parameter
* $\gamma / W \sim \chi^2_{\gamma}$

### t (Student's, Standard and General)

$X = \alpha + \beta Y$

* $X \sim$ Generalized t w/ $\gamma$ degrees of freedom
  * $\alpha$: Location parameter
  * $\beta$: Scaling parameter
* $Y \sim$ Student's t or standard t w/ $\gamma$ degrees of freedom

Exam note: Formula in the *Table* is for standard t ($\alpha = 0$ and $\beta = 1$)

CDF of the t can not be determined analytically except when $\gamma=1$ (Cauchy distribution)

t is leptokurtic (fatter tail) makes this an important distribution for risk modeling

* Kurtosis of the standard t is > than the normal

The degrees of freedom ($\gamma$) is also a shape parameter  
(determines the kurtosis)

Tails of the *t* follow a power law

* Probability of an event falling approximately in inverse proportion to the size of the event raised to the power of $\gamma + 1$
* For large $y$ you can ignore the 1 in the PDF, which is then $\propto y^{-(\gamma + 1)}$; So for the Cauchy distribution this is the inverse square of the size of the event

***Standard t as nomral mixture***

Define standard t in terms of a ratio of $N(0,1)$ and $\chi^2$

$t_{\gamma} = \dfrac{N(0,1)}{\sqrt{\chi^2_{\gamma} \big/ \gamma}}$

* Basis of the t-test

Rewrite the to has the same form as a mixture distribution

$t_{\gamma} = \underbrace{0}_{m(W)} + \sqrt{\underbrace{\dfrac{\gamma}{\chi^2_{\gamma}}}_{W}} \times \underbrace{1}_{\beta} \times N(0,1)$

* $W \sim$ inverse gamma

***Simulation***

$Y = \alpha + \beta \dfrac{Z}{\sqrt{W \big / \gamma}}$

* Normal mixture and has generalized t with parameters $\alpha$, $\beta$, and $\gamma$
* $Z \sim N(0,1)$
* $W \sim \chi^2_{\gamma}$
* $Z \perp\!\!\perp W$
* $\mathrm{VaR}(Y) \propto 1 \big / W$

***Statistical test of a sample mean***

Testing whether a sample mean ($\bar{X}$) is statistially different from the hypothesised mean ($\mu$) of the source population is performed using the test statistic:

$Z = \dfrac{\bar{X} - \mu}{s \big / \sqrt{T}}$, where $s$ is the s.d of the sample

Test statistic follows the standard t with d.f. $\gamma = T -1 $

Rewritting $Z$

$Z = \dfrac{\bar{X} - \mu}{s \big / \sqrt{T}} \Bigg / 
\sqrt{\dfrac{(T-1)s^2 \big/ \sigma^2}{T-1}}$

* $\dfrac{\bar{X} - \mu}{s \big / \sqrt{T}} \sim N(0,1)$
* $(T-1)s^2 \big/ \sigma^2 \sim \chi^2_{T-1}$
* $Z$ is a ratio of standard normal to the sqare root of a chi-sq divided by the \# of d.f.
* As noted above, that is the standard t

### Skewed t

Same parameters as the general *t* but with the addition of a skew parameter $\delta$ (general t = skewed t with $\delta = 0$)

**Comparison of $N(0,1)$, standard *t* and skewed *t* **

![](figures/figure-16.01.png)

If $\delta < 0$ the distribution is left skewed (longer lower tail); $\delta > 0$ is right skewed

***Simulation***

$Y = \alpha + \beta \dfrac{Z}{\sqrt{W \big / \gamma}} + \delta \dfrac{1}{W \big / \gamma}$

* (Normal mixture) skewed *t* with parameters $\alpha$, $\beta$, $\gamma$ and $\delta$
* Mean is $m(w) = \alpha + \delta \dfrac{1}{W \big / \gamma}$
* Variance (from the $Z$ term) $\propto 1\big/W$
* $Z \sim N(0,1)$
* $W \sim \chi^2_{\gamma}$
* $Z \perp\!\!\perp W$

### Lognormal

$Y = \ln X \sim$ Normal then $X$ is lognormal

Applications of the lognormal

* Applicable to may insurance situation since it takes only positive values
* Can be used to model financial variables (e.g. asset return) w/ assumptions that the log of the variable will follow a random walk with drift $\ln X_t = \mu + \ln X_{t-1} + \epsilon_t$ and the returns are iid

### Wald (or Inverse Gaussian)

Wald distribution describes the time taken for a Brownian motion process to reach a given value

Wald is a special case of GIG

Only takes positive values and have positive skew

It's call inverse Gaussian because its PDF has a similar form to the normal distribution but with $1/x$ included in the power

Key benefits of the Wald is its aggregation properties

* For $X \sim Wald(\alpha, \gamma)$ then $nX \sim Wald(n\alpha, n\gamma)$ for $n>0$
* For $X_n \sim Wald(\alpha w_n, \gamma w^2_n)$ and $X_i \perp\!\!\perp X_j$ then $\sum \limits_{n=1}^N X_n \sim Wald \left( \alpha \sum \limits_{n=1}^N w_n , \gamma \left(\sum \limits_{n=1}^N w_n\right)^2\right)$

### Chi-Squared

$chi^2_{\gamma}$ is the distribution of the sum of $\gamma$ $X \sim N(0,1)$ and so can be simulated as such

$chi^2_{\gamma}$ is a special case of the gamma distribution

The link between the mean ($\mu = \gamma > 0$) and variance ($\sigma^2 = 2\gamma$) limits the application of this distribution

***Chi-squared test***

Can be applied where observations can each be associated with just one of $N$ categories

* Tests examines whether a set of assumed probabilities are incorrect

Let the actual number of observations in the n^th^ category be $T_n$ and $T = \sum \limits_{n=1}^N T_n$

Let $p_n$ be the assumed probability such that the expected number of observations in category $n$ is $Tp_n$

Test statistic: $k = \left( \sum \limits_{n=1}^N X_n^2\right) \sim \chi^2_{N-1}$

* Where $X_n = \dfrac{(T_n - Tp_n)}{\sqrt{Tp_n(1-p_n)}}$

Too large a value suggest that the set of assumed probabilities is incorrect

### Exponential

Has a single scale parameter $\beta$

Distribution provides the expected waiting times between the events of a poisson process

**Limitations to ERM** due to its characteristics

* Monotonically decreasing nature
* Single parameter
* Low probabilities associated with extreme values

### Gamma and Inverse-Gamma

***Gamma***

Gamma family has 2 positive parameters and is a verstile family

* PDF can take significantly different shapes depending on the parameters

Special cases:

* Exponential is when $\gamma = 1$
* $\chi^2_{2\gamma}$ is when $\beta = 2$

Probabilities fro a gamma is not given in the *Tables* but can be obtained by turning a gamma probability into a $\chi^2$:

* $X \sim Gamma(\beta, \gamma) \equiv \dfrac{2X}{\beta} \sim \chi^2_{2\gamma}$

Aggregation properties:

* If $X_n \sim Gamma(\beta, \gamma_n)$ are all *iid* then $\sum \limits_{n=1}^N X_n \sim Gamma\left(\beta, \sum \limits_{n=1}^N \gamma_n \right)$
* If $X \sim Gamma(\beta, \gamma)$ then $nX \sim Gamma(n\beta, \gamma)$ if $n > 0$

***Inverse-Gamma***

If $Y \sim Gamma$ then $X = \dfrac{1}{Y} \sim$ Inverse-Gamma

Both gamma and inverse gamma can be fitted by equating sample and population moments and solving for the distribution's parameters

### Generalized Inverse Gaussian (GIG)

GIG offers significant flexibility with regard to its shape with 3 parameters: $\gamma$ $\beta_1$ and $\beta_2$

Special limiting cases:

* $\beta_1=0$: $Gamma(2\beta_2, \gamma)$
* $1/\beta_2 \rightarrow 0$: GIG tends to $InverseGamma(\beta_1 / 2, -\gamma)$
* $\gamma = - \frac{1}{2}$: Wald

### FrÃ©chet

Only has a single parameter

Specialy case of the generalized extreme value distribution

### Pareto

Two parameters

Key features

* Monotonically decreasing
* Tail follows a power law with the shape parameter ($\gamma$) determining the power

***Simulation***

$X = \dfrac{\beta}{U^{\frac{1}{\gamma}}}$

* $U \sim U(0,1)$

Corresponding CDF is $F(x) = 1 - \left( \dfrac{\beta}{x}\right)^{\gamma}$ for $x > \beta$, as $1 - \left( \dfrac{\beta}{\beta \big/ u^{\frac{1}{\gamma}}}\right)^{\gamma} = u$ and by definition $F(x) \sim U(0,1)$

Formual for Pareto CDF in Sweeting and *Table* actually corresponds to $X = \dfrac{\beta}{U^{\frac{1}{\gamma}}} - \beta$

### Generalized Pareto

Three parameter

* More flexible modeling tool
* Applied in extreme value theory (mod 20)

Note:

* Gamma in the Pareto corresponds to the gamma in the generalized form
* $\beta$ in the Pareto corresponds to $\beta \gamma$ in the generalized form shown in Sweeting

Relationships:

* $\gamma = 0$: Exponential
* $\gamma > 0$: Pareto

### Uniform

Assigns equal probability to all outcomes in a range $[\beta_1, \beta_2]$

Everything is in the *Tables* where $a = \beta_1$ and $b = \beta_2$

### Triangular

Can be used in cases where in additional to the upper and lower values, the most likely value is known

* Distribution has lower limit $\beta_1$, mode $\alpha$ and upper limit $\beta_2$
* Mean is the average of the parameter values $\mu = \frac{1}{3}(\beta_1 + \alpha +\beta_2)$
* Can be positively or negatively skewed

## Multivariate